* Functions for using

** Primitive functions
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./.git.sh
  P_TOUCH(){
      test -e "./${1}" || touch "./${1}"
  }

  P_GITADD(){
      git add "./${1}"
  }

  P_MKDIR(){
      mkdir -pv -- "./${1}"
  }

  P_CLEAN(){
      rm -vf -- "./${1}"
  }

  P_SET(){
      mkdir -pv -- "$('dirname' -- "./${2}")"
      cp "./${1}" "./${2}"
  }
#+end_src

** Compound functions
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./.git.sh
  G(){
      P_TOUCH "${1}"
      P_GITADD "${1}"
  }

  S(){
      G "${1}"
      P_SET "${1}" "${2}"
      G "${2}"
  }
#+end_src

** Clean the working file
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./.git.sh
  P_CLEAN './.git.sh'
#+end_src

** Add this file
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./.git.sh
  G 'README.org'
#+end_src

* Container image related functions

** Add the file to git
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./.git.sh
  G 'important_functions.sh'
#+end_src

** Actual script

*** Run container
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./important_functions.sh
  RUN_CONTAINER () {
      CMD='sudo -A docker'
      which podman && CMD='podman'
      ${CMD} run -it --rm -v "$(realpath .):/data" debtestrustzshhelixpytorch zsh
  }
#+end_src

* Python file to produce the model file

** Add the file to git
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./.git.sh
  G 'main.py'
#+end_src

** Actual file
#+begin_src python :shebang #!/usr/bin/env python3 :results output :tangle ./main.py
  import os

  try:
      __file__
  except:
      basepath = "."
  else:
      basepath = os.path.abspath(os.path.dirname(__file__) + "/")
  import sys

  sys.path.append(basepath)
  INPUT_SIZE = 100
  BATCH_SIZE = 4
  import einops
  import timm
  import torch
  from torch.export.dynamic_shapes import Dim


  def export_to_dynamo(path_file_out):
      with torch.no_grad():
          model = model_wrapper()
          model = torch.compile(
              model=model,
              fullgraph=True,
              dynamic=True,
              backend="inductor",
              mode="max-autotune",
          )
          x = torch.rand(
              (BATCH_SIZE, INPUT_SIZE),
              dtype=torch.float32,
          )
          y = model(x)

          dynamic_shapes = {
              "x": (Dim.DYNAMIC, Dim.STATIC),
          }
          exported_module = torch.export.export(
              model._orig_mod,
              (x,),
              dynamic_shapes=dynamic_shapes,
              # strict=True,
              # dynamic_shapes=dynamic_shapes,
          )

          output_path = torch._inductor.aoti_compile_and_package(
              exported_module,
              # [Optional] Specify the generated shared library path. If not specified,
              # the generated artifact is stored in your system temp directory.
              package_path=path_file_out + ".pt2",
          )

          # compiled_model = torch.compile(
          #     model=exported_module.module(),
          #     fullgraph=True,
          #     dynamic=True,
          #     backend="inductor",
          #     mode="max-autotune",
          # )

          jit_module = torch.jit.trace(
              func=exported_module.module(),
              example_inputs=x,
          )
      jit_module.save(path_file_out + ".pt")


  def export_to_onnx(path_file_out):
      model = model_wrapper()
      model = torch.compile(
          model=model,
          fullgraph=True,
          dynamic=True,
          backend="inductor",
          mode="max-autotune",
      )
      x = torch.rand(
          (BATCH_SIZE, INPUT_SIZE),
          dtype=torch.float32,
      )
      y = model(x)
      res = torch.onnx.export(
          slave,
          x,
          path_file_out,
          input_names="x",
          output_names="y",
          opset_version=23,
          dynamo=True,
          external_data=True,
      )


  class model_wrapper(torch.nn.Module):
      def __init__(self):
          super().__init__()
          self.L1 = torch.nn.Linear(
              in_features=INPUT_SIZE,
              out_features=4,
              bias=True,
              dtype=torch.float32,
          )

      def forward(
          self,
          x: torch.Tensor,
      ):
          x = self.L1(x)
          return x


  export_to_dynamo(path_file_out="out")
#+end_src

* C++ program

** Add file to git
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./.git.sh
  S 'src,main.cpp' 'src/main.cpp'
  S 'src,main.hpp' 'src/main.hpp'
  G 'compile_g++.sh'
#+end_src

** Actual file
#+begin_src c++ :tangle ./src,main.cpp
  #include <iostream>
  #include <vector>

  #include <torch/csrc/inductor/aoti_package/model_package_loader.h>
  #include <torch/torch.h>

  extern "C" {

  unsigned long constexpr INPUT_SIZE = 100;
  unsigned long constexpr OUTPUT_SIZE = 4;

  struct arg_input {
    float val[INPUT_SIZE];
  };

  struct arg_output {
    float val[OUTPUT_SIZE];
  };

  arg_output do_infer(arg_input const *in) {
    static c10::InferenceMode mode;
    static torch::inductor::AOTIModelPackageLoader loader("out.pt2");

    std::vector<torch::Tensor> inputs = {torch::zeros({1, INPUT_SIZE}, at::kCPU)};
    for (int i = 0; i < INPUT_SIZE; i++) {
      inputs[0][0][i] = in->val[i];
    }

    std::vector<torch::Tensor> outputs = loader.run(inputs);

    arg_output out;
    for (int i = 0; i < OUTPUT_SIZE; i++) {
      out.val[i] = outputs[0][0][i].item<float>();
    }

    return out;
  }
  }
#+end_src

** Header file
#+begin_src c++ :tangle ./src,main.hpp
  extern "C" {

  unsigned long constexpr INPUT_SIZE = 100;
  unsigned long constexpr OUTPUT_SIZE = 4;

  struct arg_input {
    float val[INPUT_SIZE];
  };

  struct arg_output {
    float val[OUTPUT_SIZE];
  };

  arg_output do_infer(arg_input const *);
  }
#+end_src

** Script to compile
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./compile_g++.sh
  mkdir -pv -- './lib/' './tmp/'
  g++ \
      './src/main.cpp' -o './tmp/main.o' \
      -c -fPIC \
      '-I/usr/include/torch/csrc/api/include/' \
  ;

  g++ \
      './tmp.cpp' -o './tmp.exe' \
      './tmp/main.o' \
      -ltorch \
      -ltorch_cpu \
      -lc10 \
  ;
      # -laoti_custom_ops \
      # -lbackend_with_compiler \
      # -lgomp-98b21ff3 \
      # -ljitbackend_test \
      # -lnnapi_backend \
      # -lshm \
      # -ltorch_global_deps \
      # -ltorch_python \
      # -ltorchbind_test \
#+end_src

* Cargo.toml

** Add the file to git
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./.git.sh
  G 'Cargo.toml'
#+end_src

** Actual file
#+begin_src toml :tangle ./Cargo.toml
  [package]
  name = "burn_test"
  version = "0.1.0"
  edition = "2024"

  [build-dependencies]
  cc = { version = "1.2.46", features = ["jobserver", "parallel"] }
#+end_src

* Script to run the rust code

** Add the file to git
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./.git.sh
  G 'run.sh'
#+end_src

** Actual file
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./run.sh
  cd "$(dirname -- "${0}")"
  cargo run
#+end_src

* build.rs

** Add the file to git
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./.git.sh
  G 'build.rs'
#+end_src

** Actual file
#+begin_src rust :tangle ./build.rs
  fn main() {
      let library_path = std::path::Path::new("/usr/include/torch/csrc/api/include/");

      cc::Build::new()
          .cpp(true)
          .file("src/main.cpp")
          .include(library_path)
          .compile("main");

      println!("cargo:rustc-link-arg=-ltorch");
      println!("cargo:rustc-link-arg=-lc10");
      println!("cargo:rustc-link-arg=-ltorch_cpu");
      // println!("cargo:rustc-link-arg=-Wl,--no-as-needed");
      // println!("cargo:rustc-link-arg=-laoti_custom_ops");
      // println!("cargo:rustc-link-arg=-lgomp-98b21ff3");
      // println!("cargo:rustc-link-arg=-lbackend_with_compiler");
      // println!("cargo:rustc-link-arg=-ljitbackend_test");
      // println!("cargo:rustc-link-arg=-lnnapi_backend");
      // println!("cargo:rustc-link-arg=-lshm");
      // println!("cargo:rustc-link-arg=-ltorch");
      // println!("cargo:rustc-link-arg=-ltorch_global_deps");
      // println!("cargo:rustc-link-arg=-ltorch_python");
      // println!("cargo:rustc-link-arg=-ltorchbind_test");
  }
#+end_src

* Main rust code

** Add files to git
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./.git.sh
  S 'src,main.rs' 'src/main.rs'
#+end_src

** Actual code

*** Outputs from bindgen
#+begin_src rust :tangle ./src,main.rs
  pub const INPUT_SIZE: ::std::os::raw::c_ulong = 100;
  pub const OUTPUT_SIZE: ::std::os::raw::c_ulong = 4;
  #[repr(C)]
  #[derive(Debug, Copy, Clone)]
  pub struct arg_input {
      pub val: [f32; 100usize],
  }
  // #[allow(clippy::unnecessary_operation, clippy::identity_op)]
  // const _: () = {
  //     ["Size of arg_input"][::std::mem::size_of::<arg_input>() - 400usize];
  //     ["Alignment of arg_input"][::std::mem::align_of::<arg_input>() - 4usize];
  //     ["Offset of field: arg_input::val"][::std::mem::offset_of!(arg_input, val) - 0usize];
  // };
  #[repr(C)]
  #[derive(Debug, Copy, Clone)]
  pub struct arg_output {
      pub val: [f32; 4usize],
  }
  // #[allow(clippy::unnecessary_operation, clippy::identity_op)]
  // const _: () = {
  //     ["Size of arg_output"][::std::mem::size_of::<arg_output>() - 16usize];
  //     ["Alignment of arg_output"][::std::mem::align_of::<arg_output>() - 4usize];
  //     ["Offset of field: arg_output::val"][::std::mem::offset_of!(arg_output, val) - 0usize];
  // };
  unsafe extern "C" {
      pub fn do_infer(arg1: *const arg_input) -> arg_output;
  }
#+end_src

*** Actual rust parts
#+begin_src rust :tangle ./src,main.rs
  fn main() {

      let input = arg_input {
          val: [0.0; f32],
      };

      unsafe {
      let res = do_infer(&input);
          println!("{:?}",res);
      }
  }
#+end_src

* COMMENT Work space
#+begin_src emacs-lisp :results silent
  (save-buffer)
  (org-babel-tangle)
  (async-shell-command "
      './.git.sh'
  " "log" "err")
#+end_src
