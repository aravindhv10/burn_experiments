* Scripts to manage
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./.git.sh
  . "${HOME}/important_functions.sh"
  CLEAN '#README.org#'
  CLEAN '.git.sh'
  CLEAN 'README.org~'
  GITADD 'README.org'
#+end_src

* Host scripts

** Add file to git
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./.git.sh
  GITADD 'host.image_names.sh'
#+end_src

** Important envs
#+begin_src sh :shebang #!/bin/sh :results output :mkdirp yes  :tangle ./host.image_names.sh
  IMAGE_NAME='debtestrustzshhelixpytorch'
#+end_src

** Important functions
#+begin_src sh :shebang #!/bin/sh :results output :mkdirp yes :tangle ./host.image_names.sh
  RUN_CONTAINER () {
      CMD='sudo -A docker'
      which podman && CMD='podman'
      ${CMD} run -it --rm \
          -v "$(realpath .):/data" \
          "${IMAGE_NAME}" zsh ;
  }
#+end_src

* Scripts to run image

** Add file to git
#+begin_src sh :shebang #!/bin/sh :results output :mkdirp yes :tangle ./.git.sh
  GITADD 'image_run.sh'
#+end_src

** Actual script
#+begin_src sh :shebang #!/bin/sh :results output :mkdirp yes :tangle ./image_run.sh
  cd "$(dirname -- "${0}")"
  . './host.image_names.sh'
  RUN_CONTAINER
#+end_src

* Python file to produce the model file

** Produce the model

*** Add the file to git
#+begin_src sh :shebang #!/bin/sh :results output :mkdirp yes :tangle ./.git.sh
  GITADD 'main.py'
#+end_src

*** Actual file
#+begin_src python :shebang #!/usr/bin/env python3 :results output :mkdirp yes :tangle ./main.py
  import os

  try:
      __file__
  except:
      basepath = "."
  else:
      basepath = os.path.abspath(os.path.dirname(__file__) + "/")
  import sys

  sys.path.append(basepath)
  INPUT_SIZE = 100
  BATCH_SIZE = 4
  import einops
  import timm
  import torch
  from torch.export.dynamic_shapes import Dim


  def produce_model(path_file_out):
      with torch.no_grad():
          model = model_wrapper()
          model = torch.compile(
              model=model,
              fullgraph=True,
              dynamic=True,
              backend="inductor",
              mode="max-autotune",
          )
          x = torch.rand(
              (BATCH_SIZE, INPUT_SIZE),
              dtype=torch.float32,
          )
          y = model(x)

          dynamic_shapes = {
              "x": (Dim.DYNAMIC, Dim.STATIC),
          }
          exported_module = torch.export.export(
              model._orig_mod,
              (x,),
              dynamic_shapes=dynamic_shapes,
              # strict=True,
              # dynamic_shapes=dynamic_shapes,
          )

          torch.export.save(
              ep = exported_module,
              f = path_file_out + ".pt2",
          )

          # output_path = torch._inductor.aoti_compile_and_package(
          #     exported_module,
          #     # [Optional] Specify the generated shared library path. If not specified,
          #     # the generated artifact is stored in your system temp directory.
          #     package_path=path_file_out + ".pt2",
          # )

          # compiled_model = torch.compile(
          #     model=exported_module.module(),
          #     fullgraph=True,
          #     dynamic=True,
          #     backend="inductor",
          #     mode="max-autotune",
          # )

          # jit_module = torch.jit.trace(
          #     func=exported_module.module(),
          #     example_inputs=x,
          # )
      # jit_module.save(path_file_out + ".pt")


  def export_to_onnx(path_file_out):
      model = model_wrapper()
      model = torch.compile(
          model=model,
          fullgraph=True,
          dynamic=True,
          backend="inductor",
          mode="max-autotune",
      )
      x = torch.rand(
          (BATCH_SIZE, INPUT_SIZE),
          dtype=torch.float32,
      )
      y = model(x)
      res = torch.onnx.export(
          slave,
          x,
          path_file_out,
          input_names="x",
          output_names="y",
          opset_version=23,
          dynamo=True,
          external_data=True,
      )


  class model_wrapper(torch.nn.Module):
      def __init__(self):
          super().__init__()
          self.L1 = torch.nn.Linear(
              in_features=INPUT_SIZE,
              out_features=4,
              bias=True,
              dtype=torch.float32,
          )

      def forward(
          self,
          x: torch.Tensor,
      ):
          x = self.L1(x)
          return x


  produce_model(path_file_out="model_input")
#+end_src

** Compile the model

*** Add the file to git
#+begin_src sh :shebang #!/bin/sh :results output :mkdirp yes :tangle ./.git.sh
  GITADD 'main.py'
#+end_src

*** Actual file
#+begin_src python :shebang #!/usr/bin/env python3 :results output :mkdirp yes :tangle ./compile.py
  import os
  import sys

  INPUT_SIZE = 100
  BATCH_SIZE = 4
  import einops
  import timm
  import torch
  from torch.export.dynamic_shapes import Dim


  def compile_to_dynamo(path_file_in, path_file_out):
      exported_module = torch.export.load(f=path_file_in + ".pt2")
      output_path = torch._inductor.aoti_compile_and_package(
          exported_module,
          # [Optional] Specify the generated shared library path. If not specified,
          # the generated artifact is stored in your system temp directory.
          package_path=path_file_out + ".pt2",
      )


  compile_to_dynamo(path_file_in="model_input", path_file_out="model_output")
#+end_src

* C++ program

** Add file to git
#+begin_src sh :shebang #!/bin/sh :results output :mkdirp yes :tangle ./.git.sh
  GITADD 'src/main.cpp'
  GITADD 'src/main.hpp'
  GITADD 'compile_g++.sh'
  GITADD 'compile_commands.json'
#+end_src

** Actual file
#+begin_src c++ :mkdirp yes :tangle ./src/main.cpp
  #include <torch/csrc/inductor/aoti_package/model_package_loader.h>
  #include <torch/torch.h>
  #include <vector>

  extern "C" {

  unsigned long constexpr INPUT_SIZE = 100;
  unsigned long constexpr OUTPUT_SIZE = 4;

  struct arg_input {
    float val[INPUT_SIZE];
  };

  struct arg_output {
    float val[OUTPUT_SIZE];
  };

  void do_infer(arg_input const *in, unsigned int batch_size, arg_output *out) {
    static c10::InferenceMode mode;
    static torch::inductor::AOTIModelPackageLoader loader("out.pt2");

    std::vector<torch::Tensor> inputs = {
        torch::zeros({batch_size, INPUT_SIZE}, at::kCPU)};
    for (int j = 0; j < batch_size; ++j) {
      for (int i = 0; i < INPUT_SIZE; ++i) {
        inputs[0][j][i] = in[j].val[i];
      }
    }

    std::vector<torch::Tensor> outputs = loader.run(inputs);

    for (int j = 0; j < batch_size; ++j) {
      for (int i = 0; i < OUTPUT_SIZE; i++) {
        out[j].val[i] = outputs[0][j][i].item<float>();
      }
    }
  }
  }
#+end_src

** Header file
#+begin_src c++ :mkdirp yes :tangle ./src/main.hpp
  extern "C" {

  unsigned long constexpr INPUT_SIZE = 100;
  unsigned long constexpr OUTPUT_SIZE = 4;

  struct arg_input {
    float val[INPUT_SIZE];
  };

  struct arg_output {
    float val[OUTPUT_SIZE];
  };

  void do_infer(arg_input const *in, unsigned int batch_size, arg_output *out) ;
  }
#+end_src

** Script to compile
#+begin_src sh :shebang #!/bin/sh :results output :mkdirp yes :tangle ./compile_g++.sh
  mkdir -pv -- './lib/' './tmp/'

  bear -- \
      clang++ \
      './src/main.cpp' -o './tmp/main.o' \
      -c -fPIC \
      '-I/usr/include/torch/csrc/api/include/' \
  ;

  # clang++ \
  #     './tmp.cpp' -o './tmp.exe' \
  #     './tmp/main.o' \
  #     -ltorch \
  #     -ltorch_cpu \
  #     -lc10 \
  # ;
      # -laoti_custom_ops \
      # -lbackend_with_compiler \
      # -lgomp-98b21ff3 \
      # -ljitbackend_test \
      # -lnnapi_backend \
      # -lshm \
      # -ltorch_global_deps \
      # -ltorch_python \
      # -ltorchbind_test \
#+end_src

** compile_commands for lsp
#+begin_src conf :mkdirp yes :tangle ./compile_commands.json
  [
    {
      "arguments": [
        "/usr/bin/clang++",
        "-c",
        "-fPIC",
        "-I/usr/include/torch/csrc/api/include/",
        "-o",
        "./tmp/main.o",
        "./src/main.cpp"
      ],
      "directory": "/data",
      "file": "/data/src/main.cpp",
      "output": "/data/tmp/main.o"
    }
  ]
#+end_src

* Cargo.toml

** Add the file to git
#+begin_src sh :shebang #!/bin/sh :results output :mkdirp yes :tangle ./.git.sh
  GITADD 'Cargo.toml'
#+end_src

** Actual file
#+begin_src toml :mkdirp yes :tangle ./Cargo.toml
  [package]
  name = "burn_test"
  version = "0.1.0"
  edition = "2024"

  [build-dependencies]
  cc = { version = "1.2.46", features = ["jobserver", "parallel"] }
#+end_src

* build.rs

** Add the file to git
#+begin_src sh :shebang #!/bin/sh :results output :mkdirp yes :tangle ./.git.sh
  GITADD 'build.rs'
#+end_src

** Actual file
#+begin_src rust :mkdirp yes :tangle ./build.rs
  fn main() {
      let library_path = std::path::Path::new("/usr/include/torch/csrc/api/include/");

      cc::Build::new()
          .cpp(true)
          .file("src/main.cpp")
          .include(library_path)
          .compile("main");

      println!("cargo:rustc-link-arg=-ltorch");
      println!("cargo:rustc-link-arg=-lc10");
      println!("cargo:rustc-link-arg=-ltorch_cpu");
      // println!("cargo:rustc-link-arg=-Wl,--no-as-needed");
      // println!("cargo:rustc-link-arg=-laoti_custom_ops");
      // println!("cargo:rustc-link-arg=-lgomp-98b21ff3");
      // println!("cargo:rustc-link-arg=-lbackend_with_compiler");
      // println!("cargo:rustc-link-arg=-ljitbackend_test");
      // println!("cargo:rustc-link-arg=-lnnapi_backend");
      // println!("cargo:rustc-link-arg=-lshm");
      // println!("cargo:rustc-link-arg=-ltorch");
      // println!("cargo:rustc-link-arg=-ltorch_global_deps");
      // println!("cargo:rustc-link-arg=-ltorch_python");
      // println!("cargo:rustc-link-arg=-ltorchbind_test");
  }
#+end_src

* Main rust code

** Add files to git
#+begin_src sh :shebang #!/bin/sh :results output :mkdirp yes :tangle ./.git.sh
  GITADD 'src/main.rs'
#+end_src

** Actual code

*** Outputs from bindgen
#+begin_src rust :mkdirp yes :tangle ./src/main.rs
  pub const INPUT_SIZE: ::std::os::raw::c_ulong = 100;
  pub const OUTPUT_SIZE: ::std::os::raw::c_ulong = 4;
  #[repr(C)]
  #[derive(Debug, Copy, Clone)]
  pub struct arg_input {
      pub val: [f32; 100usize],
  }
  #[allow(clippy::unnecessary_operation, clippy::identity_op)]
  const _: () = {
      ["Size of arg_input"][::std::mem::size_of::<arg_input>() - 400usize];
      ["Alignment of arg_input"][::std::mem::align_of::<arg_input>() - 4usize];
      ["Offset of field: arg_input::val"][::std::mem::offset_of!(arg_input, val) - 0usize];
  };
  #[repr(C)]
  #[derive(Debug, Copy, Clone)]
  pub struct arg_output {
      pub val: [f32; 4usize],
  }
  #[allow(clippy::unnecessary_operation, clippy::identity_op)]
  const _: () = {
      ["Size of arg_output"][::std::mem::size_of::<arg_output>() - 16usize];
      ["Alignment of arg_output"][::std::mem::align_of::<arg_output>() - 4usize];
      ["Offset of field: arg_output::val"][::std::mem::offset_of!(arg_output, val) - 0usize];
  };
  unsafe extern "C" {
      pub fn do_infer(
          in_: *const arg_input,
          batch_size: ::std::os::raw::c_uint,
          out: *mut arg_output,
      );
  }
#+end_src

*** Actual rust parts
#+begin_src rust :mkdirp yes :tangle ./src/main.rs
  fn run_inference(input: Vec<arg_input>) -> Vec<arg_output> {
      let tmp = arg_output { val: [0.0; 4usize] };

      let mut output = Vec::<arg_output>::with_capacity(input.len());

      for _ in 0..input.len() {
          output.push(tmp);
      }

      unsafe {
          do_infer(input.as_ptr(), input.len() as u32, output.as_mut_ptr());
      }

      output
  }

  fn main() {
      let mut input = arg_input {
          val: [0.0; 100usize],
      };

      for i in 0..100 {
          input.val[i] = (i as f32) / 100.0;
      }

      let vec_input = vec![input, input, input];
      let vec_output = run_inference(vec_input);
      println!("{:?}", vec_output);
  }
#+end_src

* Script to run the rust code

** Add the file to git
#+begin_src sh :shebang #!/bin/sh :results output :mkdirp yes :tangle ./.git.sh
  GITADD 'run.sh'
#+end_src

** Actual file
#+begin_src sh :shebang #!/bin/sh :results output :mkdirp yes :tangle ./run.sh
  cd "$(dirname -- "${0}")"
  cargo run
#+end_src

* COMMENT Work space
#+begin_src emacs-lisp :results silent
  (save-buffer)
  (org-babel-tangle)
  (async-shell-command "
      './.git.sh'
  " "log" "err")
#+end_src
