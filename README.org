* Scripts to manage
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./.git.sh
  . "${HOME}/important_functions.sh"
  CLEAN '#README.org#'
  CLEAN '.git.sh'
  CLEAN 'README.org~'
  GITADD 'README.org'
#+end_src

* Scripts to run image

** Add file to git
#+begin_src sh :shebang #!/bin/sh :results output :mkdirp yes :tangle ./.git.sh
  GITADD 'image_run.sh'
#+end_src

** Actual script
#+begin_src sh :shebang #!/bin/sh :results output :mkdirp yes :tangle ./image_run.sh
  cd "$(dirname -- "${0}")"

  IMAGE_NAME='debtestrustzshhelixpytorch2'

  RUN_CONTAINER () {
      CMD='sudo -A docker'
      which podman && CMD='podman'
      ${CMD} run \
          -it --rm \
          '--device' '/dev/kfd' \
          '--device' '/dev/dri' \
          '--security-opt' 'seccomp=unconfined' \
          -v "$(realpath .):/data" \
          "${IMAGE_NAME}" zsh \
      ;
  }

  RUN_CONTAINER
#+end_src

* Python file to produce the model file and compile it

** Shell scsript to update the model only based on checksum

*** Add script to git
#+begin_src sh :shebang #!/bin/sh :results output :mkdirp yes :tangle ./.git.sh
  GITADD 'main.sh'
#+end_src

*** Actual script
#+begin_src sh :shebang #!/bin/sh :results output :mkdirp yes :tangle ./main.sh
  H="$(sha512sum ./main.py | cut -d ' ' -f1)"
  test -e "${H}.pt2" && exit '0'
  ./main.py "${H}.pt2"
  ln -vfs -- "${H}.pt2" "./model.pt2"
  exit '0'
#+end_src

** Produce the model

*** Add the file to git
#+begin_src sh :shebang #!/bin/sh :results output :mkdirp yes :tangle ./.git.sh
  PREPARE_PYTHON_FILE 'main'
#+end_src

*** Actual file

**** config the import paths
#+begin_src python :shebang #!/usr/bin/env python3 :results output :mkdirp yes :tangle ./main.config.py
  import os

  try:
      __file__
  except:
      basepath = "."
  else:
      basepath = os.path.abspath(os.path.dirname(__file__) + "/")
  import sys

  sys.path.append(basepath)
#+end_src

**** config standard variables
#+begin_src python :shebang #!/usr/bin/env python3 :results output :mkdirp yes :tangle ./main.config.py
  IMAGE_RESOLUTION = 448
  BATCH_SIZE = 4
  NUM_CHANNELS = 3
  NUM_CLASSES = 3

  (
      SIZE_B,
      SIZE_Y,
      SIZE_X,
      SIZE_C,
  ) = (
      BATCH_SIZE,
      IMAGE_RESOLUTION,
      IMAGE_RESOLUTION,
      NUM_CHANNELS,
  )

  INPUT_SHAPE = (
      SIZE_B,
      SIZE_Y,
      SIZE_X,
      SIZE_C,
  )
#+end_src

**** import important libraries
#+begin_src python :shebang #!/usr/bin/env python3 :results output :mkdirp yes :tangle ./main.import.py
  import os
  import sys
  import einops
  import timm
  import torch
  from torch.export.dynamic_shapes import Dim
#+end_src

**** class to define the actual model
#+begin_src python :shebang #!/usr/bin/env python3 :results output :mkdirp yes :tangle ./main.class.py
  class model_wrapper(torch.nn.Module):
      ################################################################
      ## Forward related functions BEGIN #############################
      ################################################################

      def forward_1_rearrange(
          self,
          x: torch.Tensor,
      ):
          x = einops.rearrange(
              x,
              "B Y X C -> B C Y X",
          )
          return x

      def forward_2_normalize(
          self,
          x: torch.Tensor,
      ):
          for i in range(SIZE_C):
              x[:, i, :, :] = ((x[:, i, :, :] / 255.0) - self.mean[i]) / self.std[i]

          return x

      def forward_3_backbone(
          self,
          x: torch.Tensor,
      ):
          x = self.timm_model(x)
          return x

      def forward_4_postprocess(
          self,
          x: torch.Tensor,
      ):
          x = torch.nn.functional.softmax(
              x,
              dim=1,
          )
          return x

      def forward(
          self,
          x: torch.Tensor,
      ):
          x = self.forward_1_rearrange(x)
          x = self.forward_2_normalize(x)
          x = self.forward_3_backbone(x)
          x = self.forward_4_postprocess(x)
          return x

      ################################################################
      ## Forward related functions END ###############################
      ################################################################

      ################################################################
      ## Init related functions BEGIN ################################
      ################################################################

      def init_setup_stat_parameters_as_float(self):
          self.mean = (
              0.48145466,
              0.4578275,
              0.40821073,
          )
          self.std = (
              0.26862954,
              0.26130258,
              0.27577711,
          )

      def init_timm_model(self):
          self.timm_model = timm.create_model(
              "timm/eva02_base_patch14_448.mim_in22k_ft_in1k",
              num_classes=NUM_CLASSES,
              pretrained=True,
          )

      def __init__(self):
          super().__init__()
          self.init_setup_stat_parameters_as_float()
          self.init_timm_model()

      ################################################################
      ## Init related functions END ##################################
      ################################################################
#+end_src

**** function to produce the export
#+begin_src python :shebang #!/usr/bin/env python3 :results output :mkdirp yes :tangle ./main.function.py
  def produce_model(path_file_out):
      model = model_wrapper()
      model.eval()
      with torch.inference_mode():
          inductor_configs = {}
          if torch.cuda.is_available():
              device = "cuda"
              inductor_configs["max_autotune"] = True
          else:
              device = "cpu"

          print("device = ", device)

          model = model.to(device=device)
          x = torch.rand(
              INPUT_SHAPE,
              dtype=torch.float32,
              device=device,
          )

          dynamic_shapes = {
              "x": (
                  Dim.DYNAMIC,
                  Dim.STATIC,
                  Dim.STATIC,
                  Dim.STATIC,
              ),
          }

          exported_program = torch.export.export(
              # model._orig_mod,
              model,
              (x,),
              dynamic_shapes=dynamic_shapes,
              strict=True,
          )

          path = torch._inductor.aoti_compile_and_package(
              exported_program,
              package_path=path_file_out,
              inductor_configs=inductor_configs,
          )
#+end_src

**** function to test if it works
#+begin_src python :shebang #!/usr/bin/env python3 :results output :mkdirp yes :tangle ./main.function.py
  def test_model(path_file_in):
      device = "cuda"
      model = torch._inductor.aoti_load_package(path_file_in)
      x = torch.rand(
          (
              SIZE_B * 2,
              SIZE_Y,
              SIZE_X,
              SIZE_C,
          ),
          dtype=torch.float32,
          device=device,
      )
      with torch.inference_mode():
          output = model(x)

          print(output)
      return output
#+end_src

**** execute
#+begin_src python :shebang #!/usr/bin/env python3 :results output :mkdirp yes :tangle ./main.execute.py
  produce_model(path_file_out=sys.argv[1])
  test_model(path_file_in=sys.argv[1])
#+end_src

* C++ program

** Header file

*** Add the file to git
#+begin_src sh :shebang #!/bin/sh :results output :mkdirp yes :tangle ./.git.sh
  GITADD 'src/export.hpp'
#+end_src

*** Actual file
#+begin_src c++ :mkdirp yes :tangle ./src/export.hpp
  extern "C" {

  unsigned int constexpr IMAGE_RESOLUTION = 448;
  unsigned int constexpr NUM_CHANNELS = 3;
  unsigned int constexpr NUM_CLASSES = 3;

  unsigned int constexpr SIZE_Y = IMAGE_RESOLUTION;
  unsigned int constexpr SIZE_X = IMAGE_RESOLUTION;
  unsigned int constexpr SIZE_C = NUM_CHANNELS;
  unsigned int constexpr SIZE_O = NUM_CLASSES;

  typedef float intype;
  typedef float outtype;

  struct arg_input {
    intype val[SIZE_Y][SIZE_X][SIZE_C];
  };

  struct arg_output {
    outtype val[SIZE_O];
  };

  void mylibtorchinfer(arg_input *in, unsigned int const batch_size, arg_output *out);
  }
#+end_src

** Include all the main headers

*** Add the file to git
#+begin_src sh :shebang #!/bin/sh :results output :mkdirp yes :tangle ./.git.sh
  GITADD 'src/main.hpp'
#+end_src

*** Actual file
#+begin_src c++ :mkdirp yes :tangle ./src/main.hpp
  #include <torch/csrc/inductor/aoti_package/model_package_loader.h>
  #include <torch/torch.h>
  #include <vector>
  #include <iostream>
  #include "./export.hpp"
#+end_src

** Main C++ code

*** Add the file to git
#+begin_src sh :shebang #!/bin/sh :results output :mkdirp yes :tangle ./.git.sh
  GITADD 'src/main.cpp'
#+end_src

*** Actual file
#+begin_src c++ :mkdirp yes :tangle ./src/main.cpp
  #include "./main.hpp"

  class infer_slave {
    c10::InferenceMode mode;
    torch::inductor::AOTIModelPackageLoader loader;
    torch::TensorOptions options;
    torch::Tensor input_tensor;
    std::vector<torch::Tensor> inputs;
    std::vector<torch::Tensor> outputs;
    torch::Tensor out_tensor;
    std::size_t bytes_to_copy;

  public:
    inline void operator()(arg_input *in, unsigned int const batch_size,
                           arg_output *out) {

      torch::Tensor cpu_tensor = torch::from_blob(static_cast<void *>(in), {batch_size, SIZE_Y, SIZE_X, SIZE_C}, torch::kCPU);

      inputs[0] = cpu_tensor.to(options);
      outputs = loader.run(inputs);
      out_tensor = outputs[0].contiguous().cpu();
      bytes_to_copy = batch_size * SIZE_O * sizeof(outtype);
      std::memcpy(out, out_tensor.data_ptr<outtype>(), bytes_to_copy);
    }

    infer_slave()
        : loader("model.pt2"),
          options(
              torch::TensorOptions().dtype(torch::kFloat32).device(torch::kCUDA)) {
      inputs.resize(1);
    }

    ~infer_slave() {}
  };

  infer_slave slave;

  extern "C" {

  void mylibtorchinfer(arg_input *in, unsigned int const batch_size,
                       arg_output *out) {

    slave(in,batch_size,out);
  }
  }
#+end_src

** Include main function to test

*** Add the file to git
#+begin_src sh :shebang #!/bin/sh :results output :mkdirp yes :tangle ./.git.sh
  GITADD './src/test.cpp'
#+end_src

*** Call the function
#+begin_src c++ :mkdirp yes :tangle ./src/test.cpp
  #include "./main.cpp"
  int main(){
    arg_input in;
    arg_output out;
    mylibtorchinfer(/*arg_input *in =*/ &in, /*unsigned int const batch_size =*/ 1, /*arg_output *out =*/ &out);
    for(int i = 0; i< NUM_CLASSES; ++i){printf("%lf, ",out.val[i]);}
    printf("\n");
    
    return 0;
  }
#+end_src

** CMake parts

*** Add the file to git
#+begin_src sh :shebang #!/bin/sh :results output :mkdirp yes :tangle ./.git.sh
  GITADD 'CMakeLists.txt'
#+end_src

*** Actual file
#+begin_src conf :mkdirp yes :tangle ./CMakeLists.txt
  cmake_minimum_required(VERSION 3.18 FATAL_ERROR)
  project(aoti_example)

  find_package(Torch REQUIRED)

  add_executable(aoti_example src/test.cpp model.pt2)

  add_custom_command(
      OUTPUT model.pt2
      COMMAND python3 ${CMAKE_CURRENT_SOURCE_DIR}/main.py
      DEPENDS main.py
  )

  target_link_libraries(aoti_example "${TORCH_LIBRARIES}")
  set_property(TARGET aoti_example PROPERTY CXX_STANDARD 17)
#+end_src

** Makefile to compile the c++ side

*** Add the file to git
#+begin_src sh :shebang #!/bin/sh :results output :mkdirp yes :tangle ./.git.sh
  GITADD 'Makefile'
#+end_src

*** Actual file

**** Optimized
#+begin_src conf :tangle ./Makefile
  CC=clang++

  install: all
  	install -C build/libmytorch.so /lib/libmytorch.so

  all: build/libmytorch.so model_output.pt2 src/export.rs
  	echo Done building all

  src/main.hpp: src/export.hpp
  	touch src/main.hpp

  src/all.hpp: src/main.hpp
  	$(CC) src/main.hpp -o src/all.hpp -E -I/usr/include/torch/csrc/api/include/

  src/main.cpp: src/all.hpp
  	touch src/main.cpp

  build/main.o: src/main.cpp
  	mkdir -pv -- ./build
  	$(CC) src/main.cpp -fPIC -c -o build/main.o -O3 -march=x86-64-v3 -mtune=native

  build/libmytorch.so: build/main.o
  	$(CC) build/main.o -o build/libmytorch.so -fPIC -shared -flto -L/lib/intel64  -L/lib/intel64_win  -L/lib/win-x64  -Wl,-rpath,/lib/intel64:/lib/intel64_win:/lib/win-x64:/opt/rocm-6.4.4/lib:/opt/rocm/lib /usr/lib/libtorch.so /usr/lib/libc10.so /usr/lib/libkineto.a -Wl,--no-as-needed,"/usr/lib/libtorch_cpu.so" -Wl,--as-needed -Wl,--no-as-needed,"/usr/lib/libtorch_hip.so" -Wl,--as-needed /usr/lib/libc10_hip.so /usr/lib/libc10.so /opt/rocm-6.4.4/lib/libMIOpen.so.1.0.60404 /opt/rocm/lib/libhiprtc.so.6.4.60404 -ldl /opt/rocm-6.4.4/lib/libhipblas.so.2.4.60404 /opt/rocm-6.4.4/lib/libhipfft.so.0.1.60404 /opt/rocm-6.4.4/lib/libhiprand.so.1.1.60404 /opt/rocm-6.4.4/lib/librocrand.so.1.1.60404 /opt/rocm-6.4.4/lib/libhipsparse.so.1.1.0.60404 /opt/rocm-6.4.4/lib/libhipsolver.so.0.4.60404 /opt/rocm-6.4.4/lib/librocsolver.so.0.4.60404 /opt/rocm-6.4.4/lib/librocblas.so.4.4.60404 /opt/rocm-6.4.4/lib/libhipblaslt.so.0.12.60404 /opt/rocm/lib/libamdhip64.so.6.4.60404 /opt/rocm-6.4.4/lib/libhipsparselt.so.0.2.60404 -Wl,--no-as-needed,"/usr/lib/libtorch.so" -Wl,--as-needed 

  clean:
  	rm -rf -- build src/all.hpp target

  model_output.pt2: model_input.pt2 compile.py
  	./compile.sh
  	touch ./model_output.pt2 

  src/export.rs: src/export.hpp
  	bindgen src/export.hpp > src/export.rs

  model_input.pt2: main.py
  	./main.sh
  	touch ./model_input.pt2
#+end_src

** Script to compile

*** Add the file to git
#+begin_src sh :shebang #!/bin/sh :results output :mkdirp yes :tangle ./.git.sh
  GITADD 'compile_g++.sh'
#+end_src

*** Actual file
#+begin_src sh :shebang #!/bin/sh :results output :mkdirp yes :tangle ./compile_g++.sh
  mkdir -pv -- './lib/' './tmp/'

  bear -- \
      clang++ \
      './src/main.cpp' -o './tmp/main.o' \
      -c -fPIC \
      '-I/usr/include/torch/csrc/api/include/' \
  ;
#+end_src

** compile_commands for lsp

*** Add the file to git
#+begin_src sh :shebang #!/bin/sh :results output :mkdirp yes :tangle ./.git.sh
  GITADD 'compile_commands.json'
#+end_src

*** Actual file
#+begin_src conf :mkdirp yes :tangle ./compile_commands.json
  [
    {
      "arguments": [
        "/usr/bin/clang++",
        "-c",
        "-fPIC",
        "-I/usr/include/torch/csrc/api/include/",
        "-o",
        "./tmp/main.o",
        "./src/main.cpp"
      ],
      "directory": "/data",
      "file": "/data/src/main.cpp",
      "output": "/data/tmp/main.o"
    }
  ]
#+end_src

* Cargo.toml

** Add the file to git
#+begin_src sh :shebang #!/bin/sh :results output :mkdirp yes :tangle ./.git.sh
  GITADD 'Cargo.toml'
#+end_src

** Actual file

*** Main metadata
#+begin_src toml :mkdirp yes :tangle ./Cargo.toml
  [package]
  name = "burn_test"
  version = "0.1.0"
  edition = "2024"

  include = [
      "src/main.cpp",
      "src/main.hpp",
      "src/main.h",
      "src/main.rs",
      "Cargo.toml",
      "Makefile",
      "build.rs",
      "compile.py",
  ]
#+end_src

*** List of files to compile

**** Server
#+begin_src toml :mkdirp yes :tangle ./Cargo.toml
  [[bin]]
  name = "infer-server"
  path = "src/main.rs"
#+end_src

**** GRPC Client
#+begin_src toml :mkdirp yes :tangle ./Cargo.toml
  [[bin]]
  name = "infer-client"
  path = "src/client.rs"
#+end_src

*** List of build dependencies
#+begin_src toml :mkdirp yes :tangle ./Cargo.toml
  [build-dependencies]
  tonic-prost-build = "0.14.2"
#+end_src

*** List of dependencies
#+begin_src toml :mkdirp yes :tangle ./Cargo.toml
  [dependencies]
  actix-multipart = "0.7.2"
  actix-web = "4.11.0"
  futures-util = "0.3.31"
  image = { version = "0.25.6", features = ["serde", "nasm"] }
  prost = "0.14"
  serde = { version = "1.0.219", features = ["derive"] }
  tokio = { version = "1.47.1", features = ["full"] }
  tonic-prost = "*"
  tonic = { version = "0.14.2", features = ["zstd"] }
#+end_src

* proto file

** Add the file
#+begin_src sh :shebang #!/bin/sh :results output :mkdirp yes :tangle ./.git.sh
  GITADD 'infer.proto'
#+end_src

** Actual file
#+begin_src rust :tangle ./infer.proto
  syntax = "proto3";

  package infer;

  message Image {
      bytes image_data = 1;
  }

  message Prediction {
      float ps1 = 1;
      float ps2 = 2;
      float ps3 = 3;
  }

  service Infer {
    rpc doInfer(Image) returns (Prediction) {}
  }
#+end_src

* build.rs

** Add the file to git
#+begin_src sh :shebang #!/bin/sh :results output :mkdirp yes :tangle ./.git.sh
  GITADD 'build.rs'
#+end_src

** Actual file
#+begin_src rust :mkdirp yes :tangle ./build.rs
  fn main() -> Result<(), Box<dyn std::error::Error>> {
      tonic_prost_build::compile_protos("./infer.proto")?;
      println!("cargo:rustc-link-arg=-lmytorch");
      Ok(())
  }
#+end_src

* GRPC inference code

** Add files to git
#+begin_src sh :shebang #!/bin/sh :results output :mkdirp yes :tangle ./.git.sh
  GITADD 'src/client.rs'
#+end_src

** Code for client
#+begin_src rust :mkdirp yes :tangle ./src/client.rs
  pub mod infer {
      tonic::include_proto!("infer"); // The string specified here must match the proto package name
  }

  // use std::error::Error;
  use std::fs;

  #[tokio::main]
  async fn main() -> Result<(), Box<dyn std::error::Error>> {
      let data = fs::read("./image.jpg").expect("Failed reading image file");
      let img = infer::Image { image_data: data };
      let mut client = infer::infer_client::InferClient::connect("http://127.0.0.1:8001").await?;
      let res = client.do_infer(img).await?;
      println!("{:?}", res);
      return Ok(());
  }
#+end_src

* Main rust code

** Add files to git
#+begin_src sh :shebang #!/bin/sh :results output :mkdirp yes :tangle ./.git.sh
  GITADD 'src/export.rs'
  GITADD 'src/main.rs'
  GITADD 'src/model.rs'
  GITADD 'src/mylib.rs'
#+end_src

** Actual code

*** lib file
#+begin_src rust :mkdirp yes :tangle ./src/mylib.rs
  pub struct image_processor {
      image_resolution: u32,
  }

  impl image_processor {

      pub fn new(val: u32) -> Self {
          return image_processor {
              image_resolution: val,
          };
      }

      fn preprocess(&self, img:  image::DynamicImage) -> image::RgbaImage {
          let (width, height) = (img.width(), img.height());
          let size = width.min(height);
          let x = (width - size) / 2;
          let y = (height - size) / 2;
          let cropped_img = image::imageops::crop_imm(&img, x, y, size, size).to_image();
          image::imageops::resize(
              &cropped_img,
              self.image_resolution,
              self.image_resolution,
              image::imageops::FilterType::CatmullRom,
          )
      }

      pub fn decode_and_preprocess(&self, data: Vec<u8>) -> Result<image::RgbaImage, String> {
          match image::load_from_memory(&data) {
              Ok(img) => {
                  return Ok(self.preprocess(img));
              }
              Err(e) => {
                  return Err("decode error".to_string());
              }
          };
      }
  }
#+end_src

*** Model code

**** Outputs from bindgen
#+begin_src rust :mkdirp yes :tangle ./src/model.rs
  // #[link(name="mytorch")]
  include!("export.rs");
#+end_src

**** Implement methods for input class
#+begin_src rust :mkdirp yes :tangle ./src/model.rs
  impl arg_input {
      pub fn new() -> Self {
          arg_input {
              val: [[[0.0; SIZE_C as  usize]; SIZE_X as usize]; SIZE_X as usize],
          }
      }
  }

  impl Default for arg_input {
      fn default() -> Self {
          arg_input::new()
      }
  }
#+end_src

**** Implement methods for output class

***** List of output labels
#+begin_src rust :mkdirp yes :tangle ./src/model.rs
  const CLASS_LABELS: [&str; SIZE_O as usize] = ["empty", "occupied", "other"];
#+end_src

***** Actual functions
#+begin_src rust :mkdirp yes :tangle ./src/model.rs
  impl arg_output {

      pub fn new() -> Self {
          arg_output {
              val: [0.0; SIZE_O as usize],
          }
      }

      pub fn from<T: std::ops::Index<usize, Output = outtype>>(input: T) -> Self {
          let mut ret = arg_output::new();
          for i in 0..SIZE_O {
              ret.val[i as usize] = input[i as usize];
          }
          ret
      }

  }

  impl Default for arg_output {
      fn default() -> Self {
          arg_output::new()
      }
  }
#+end_src

**** Main inference function
#+begin_src rust :mkdirp yes :tangle ./src/model.rs
  async fn run_inference(mut input: Vec<arg_input>) -> Vec<arg_output> {

      println!("Inside run_inference");

      let mut output: Vec<arg_output> = (0..input.len()).map(|_|{arg_output::new()}).collect(); 

      unsafe {
          println!("Calling the c++ wrapper");
          mylibtorchinfer(input.as_mut_ptr(), input.len() as u32, output.as_mut_ptr());
      }

      output
  }
#+end_src

**** Reply class

***** Actual structure
#+begin_src rust :mkdirp yes :tangle ./src/model.rs
  #[derive(serde::Serialize)]
  pub struct prediction_probabilities_reply {
      val: [String; SIZE_O as usize],
      mj: String,
  }
#+end_src

***** Methods
#+begin_src rust :mkdirp yes :tangle ./src/model.rs
  impl prediction_probabilities_reply {
      pub fn new() -> Self {
          prediction_probabilities_reply {
              val: std::array::from_fn(|_| String::new()),
              mj: String::new(),
          }
      }

      pub fn from(input: arg_output) -> prediction_probabilities_reply {
          let mut max_index: usize = 0;
          let mut ret = prediction_probabilities_reply::new();

          ret.val[0] = input.val[0].to_string();
          for i in 1..SIZE_O {
              ret.val[i as usize] = input.val[i as usize].to_string();
              if input.val[i as usize] > input.val[max_index] {
                  max_index = i as usize;
              }
          }

          ret.mj = CLASS_LABELS[max_index].to_string();

          return ret;
      }
  }
#+end_src

**** Struct to send the inference request to the inferring thread
#+begin_src rust :mkdirp yes :tangle ./src/model.rs
  pub struct InferRequest {
      img: image::RgbaImage,
      resp_tx: tokio::sync::oneshot::Sender<Result<arg_output, String>>,
  }
#+end_src

**** Model server

***** Actual struct
#+begin_src rust :mkdirp yes :tangle ./src/model.rs
  pub struct model_server {
      rx: tokio::sync::mpsc::Receiver<InferRequest>,
  }
#+end_src

***** Important configs
#+begin_src rust :mkdirp yes :tangle ./src/model.rs
  const MAX_BATCH: usize = 16;
  const BATCH_TIMEOUT: std::time::Duration = std::time::Duration::from_millis(200);
#+end_src

***** Methods
#+begin_src rust :mkdirp yes :tangle ./src/model.rs
  impl model_server {
      pub async fn infer_loop(&mut self) {
          while let Some(first) = self.rx.recv().await {
              let mut batch = vec![first];
              let start = tokio::time::Instant::now();
              while batch.len() < MAX_BATCH && start.elapsed() < BATCH_TIMEOUT {
                  match self.rx.try_recv() {
                      Ok(req) => batch.push(req),
                      Err(_) => break,
                  }
              }
              let batch_size = batch.len();

              let mut input: Vec<arg_input> = (0..batch_size).map(|_|{arg_input::new()}).collect(); 
              
              for (B, req) in batch.iter().enumerate() {
                  for (X, Y, pixel) in req.img.enumerate_pixels() {
                      let [r, g, b, _] = pixel.0;
                      input[B as usize].val[Y as usize][X as usize][0] = r as f32;
                      input[B as usize].val[Y as usize][X as usize][1] = g as f32;
                      input[B as usize].val[Y as usize][X as usize][2] = b as f32;
                  }
              }
              let outputs = run_inference(input).await ;

              for (out, req) in outputs.into_iter().zip(batch.into_iter()) {
                  let _ = req.resp_tx.send(Ok(out));
              }
          }
      }
  }
#+end_src

**** The model client

***** The Struct
#+begin_src rust :mkdirp yes :tangle ./src/model.rs
  pub struct model_client {
      tx: tokio::sync::mpsc::Sender<InferRequest>,
      preprocess: crate::mylib::image_processor,
  }
#+end_src

***** Implement the methods
#+begin_src rust :mkdirp yes :tangle ./src/model.rs
  impl model_client {
      pub async fn do_infer(
          &self,
          img: image::RgbaImage,
      ) -> Result<arg_output, String> {

          let (resp_tx, resp_rx) = tokio::sync::oneshot::channel();

          match self.tx.send(InferRequest { img, resp_tx}).await {
              Ok(_) => match resp_rx.await {
                  Ok(Ok(pred)) => {
                      return Ok(pred);
                  }
                  Ok(Err(e)) => {
                      return Err(e);
                  }
                  Err(e) => {
                      return Err("Recv Error".to_string());
                  }
              },
              Err(e) => {
                  return Err("Send error".to_string());
              }
          }
      }

      pub async fn do_infer_data(&self, data: Vec<u8>) -> Result<arg_output, String> {

          match self.preprocess.decode_and_preprocess(data) {
              Ok(img) => {
                  return self.do_infer(img).await;
              }
              Err(e) => {
                  return Err("Failed to decode and pre-process the image".to_string());
              }
          }
      }
  }
#+end_src

**** Convenient method to get both the server and client
#+begin_src rust :mkdirp yes :tangle ./src/model.rs
  pub fn get_inference_tuple() -> (model_server, model_client) {
      let (tx, rx) = tokio::sync::mpsc::channel::<InferRequest>(512);
      let ret_server = model_server {
          rx: rx,
      };
      let ret_client = model_client {
          tx: tx,
          preprocess: crate::mylib::image_processor::new(IMAGE_RESOLUTION),
      };
      return (ret_server, ret_client);
  }
#+end_src

*** Main code

**** Fixed
#+begin_src rust :mkdirp yes :tangle ./src/main.rs
  mod model;
  mod mylib;

  use futures_util::TryStreamExt;

  async fn infer_handler(
      mut payload: actix_multipart::Multipart,
      infer_slave: actix_web::web::Data<std::sync::Arc<model::model_client>>,
  ) -> Result<actix_web::HttpResponse, actix_web::Error> {

      let mut data = Vec::new();

      while let Some(mut field) = payload.try_next().await? {
          while let Some(chunk) = field.try_next().await? {
              data.extend_from_slice(&chunk);
          }
      }

      if data.is_empty() {
          return Ok(actix_web::HttpResponse::BadRequest().body("No image data"));
      }

      match infer_slave.do_infer_data(data).await {
          Ok(pred) => {
              return Ok(actix_web::HttpResponse::Ok().json(model::prediction_probabilities_reply::from(pred)));
          },
          Err(e) => {
              return Ok(actix_web::HttpResponse::InternalServerError().body(e));
          },
      }
  }

  pub mod infer {
      tonic::include_proto!("infer");
  }

  pub struct MyInferer {
      slave_client: std::sync::Arc<model::model_client>
  }

  #[tonic::async_trait]
  impl infer::infer_server::Infer for MyInferer {

      async fn do_infer(&self, request: tonic::Request<infer::Image>) -> Result<tonic::Response<infer::Prediction>, tonic::Status> {
          println!("Received gRPC request");
          let image_data = request.into_inner().image_data;
          println!("Calling the inference function");
          match self.slave_client.do_infer_data(image_data).await {
              Ok(pred) => {
                  let reply = infer::Prediction {
                      ps1: pred.val[0],
                      ps2: pred.val[1],
                      ps3: pred.val[2],
                  };
                  return Ok(tonic::Response::new(reply));
              },
              Err(e) => {
                  Err(tonic::Status::internal(e))
              },
          }
      }
  }
#+end_src

**** Actual functions to start actix and tonic

***** Actix
#+begin_src rust :mkdirp yes :tangle ./src/main.rs
  async fn main_actix(slave_client_1: std::sync::Arc<crate::model::model_client>) -> () {
      match actix_web::HttpServer::new(
          move || {
            actix_web::App::new()
                .app_data(actix_web::web::Data::new(std::sync::Arc::clone(&slave_client_1)))
                .route("/infer", actix_web::web::post().to(infer_handler))
          }
      ).bind(("0.0.0.0", 8000)) {
          Ok(ret) => {
              ret.run().await;
          }
          Err(e) => {
              eprintln!("Failed to bind to port");
          }
      }
  }
#+end_src

***** Tonic
#+begin_src rust :mkdirp yes :tangle ./src/main.rs
  async fn main_tonic(slave_client_2: std::sync::Arc<crate::model::model_client>) {
      let ip_v4 = std::net::IpAddr::V4(std::net::Ipv4Addr::new(0, 0, 0, 0));
      let addr = std::net::SocketAddr::new(ip_v4, 8001);
      let inferer_service = MyInferer{slave_client: slave_client_2};
      tonic::transport::Server::builder().add_service(infer::infer_server::InferServer::new(inferer_service)).serve(addr).await;
  }
#+end_src

**** Simpler main
#+begin_src rust :mkdirp yes :tangle ./src/main.rs
  fn main() {
      let (mut slave_server, slave_client) = crate::model::get_inference_tuple();

      let rt = tokio::runtime::Builder::new_multi_thread()
          .thread_stack_size(1 << 24) 
          .enable_all()
          .build()
          .unwrap();

      rt.block_on(async {
          let slave_client_1 = std::sync::Arc::new(slave_client);
          let slave_client_2 = std::sync::Arc::clone(&slave_client_1);
          let future_infer = slave_server.infer_loop();
          let future_actix = main_actix(slave_client_1);
          let future_tonic = main_tonic(slave_client_2);
          tokio::join!(future_infer, future_actix, future_tonic);
      });
  }
#+end_src

* Script to run the rust code

** Add the file to git
#+begin_src sh :shebang #!/bin/sh :results output :mkdirp yes :tangle ./.git.sh
  GITADD 'run.sh'
  GITADD 'infer.sh'
#+end_src

** Run server
#+begin_src sh :shebang #!/bin/sh :results output :mkdirp yes :tangle ./run.sh
  cd "$(dirname -- "${0}")"
  make -j install
  export RUSTFLAGS="-C target-cpu=native"
  export ROCR_VISIBLE_DEVICES=1
  cargo run --bin infer-server --release
#+end_src

** Run Client
#+begin_src sh :shebang #!/bin/sh :results output :mkdirp yes :tangle ./infer.sh
  cd "$(dirname -- "${0}")"
  make -j all
  export RUSTFLAGS="-C target-cpu=native"
  cargo run --bin infer-client --release
#+end_src

* COMMENT Work space
#+begin_src emacs-lisp :results silent
  (save-buffer)
  (org-babel-tangle)
  (async-shell-command "
      './.git.sh'
  " "log" "err")
#+end_src
