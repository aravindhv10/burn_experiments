* Scripts to manage
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./.git.sh
  . "${HOME}/important_functions.sh"
  CLEAN '#README.org#'
  CLEAN '.git.sh'
  CLEAN 'README.org~'
  GITADD 'README.org'
#+end_src

* Host scripts

** Add file to git
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./.git.sh
  GITADD 'host.image_names.sh'
#+end_src

** Important envs
#+begin_src sh :shebang #!/bin/sh :results output :mkdirp yes  :tangle ./host.image_names.sh
  IMAGE_NAME='burnexps'
#+end_src

** Important functions
#+begin_src sh :shebang #!/bin/sh :results output :mkdirp yes :tangle ./host.image_names.sh
  RUN_CONTAINER () {
      CMD='sudo -A docker'
      which podman && CMD='podman'
      ${CMD} run -it --rm \
          -v "$(realpath .):/data" \
          "${IMAGE_NAME}" zsh ;
  }
#+end_src

* Scripts to run image

** Add file to git
#+begin_src sh :shebang #!/bin/sh :results output :mkdirp yes :tangle ./.git.sh
  GITADD 'image_run.sh'
#+end_src

** Actual script
#+begin_src sh :shebang #!/bin/sh :results output :mkdirp yes :tangle ./image_run.sh
  cd "$(dirname -- "${0}")"
  . './host.image_names.sh'
  RUN_CONTAINER
#+end_src

* Python file to produce the model file and compile it

** Produce the model

*** Add the file to git
#+begin_src sh :shebang #!/bin/sh :results output :mkdirp yes :tangle ./.git.sh
  PREPARE_PYTHON_FILE 'main'
#+end_src

*** Actual file

**** config the import paths
#+begin_src python :shebang #!/usr/bin/env python3 :results output :mkdirp yes :tangle ./main.config.py
  import os

  try:
      __file__
  except:
      basepath = "."
  else:
      basepath = os.path.abspath(os.path.dirname(__file__) + "/")
  import sys

  sys.path.append(basepath)
#+end_src

**** config standard variables
#+begin_src python :shebang #!/usr/bin/env python3 :results output :mkdirp yes :tangle ./main.config.py
  IMAGE_RESOLUTION = 448
  BATCH_SIZE = 4
  NUM_CHANNELS = 3

  SIZE_B = BATCH_SIZE
  SIZE_Y = IMAGE_RESOLUTION
  SIZE_X = IMAGE_RESOLUTION
  SIZE_C = NUM_CHANNELS

  INPUT_SHAPE = (SIZE_B, SIZE_Y, SIZE_X, SIZE_C)
#+end_src

**** import important libraries
#+begin_src python :shebang #!/usr/bin/env python3 :results output :mkdirp yes :tangle ./main.import.py
  import einops
  import timm
  import torch
  from torch.export.dynamic_shapes import Dim
#+end_src

**** class to define the actual model
#+begin_src python :shebang #!/usr/bin/env python3 :results output :mkdirp yes :tangle ./main.class.py
  class model_wrapper(torch.nn.Module):
      ################################################################
      ## Forward related functions BEGIN #############################
      ################################################################

      def forward_1_rearrange(
          self,
          x: torch.Tensor,
      ):
          x = einops.rearrange(
              x,
              "B Y X C -> B C Y X",
          )
          return x

      def forward_2_normalize(
          self,
          x: torch.Tensor,
      ):
          for i in range(SIZE_C):
              x[:, i, :, :] = ((x[:, i, :, :] / 255.0) - self.mean[i]) / self.std[i]

          return x

      def forward_3_backbone(
          self,
          x: torch.Tensor,
      ):
          x = self.timm_model(x)
          return x

      def forward_4_postprocess(
          self,
          x: torch.Tensor,
      ):
          x = torch.nn.functional.softmax(
              x,
              dim=1,
          )
          return x

      def forward(
          self,
          x: torch.Tensor,
      ):
          x = self.forward_1_rearrange(x)
          x = self.forward_2_normalize(x)
          x = self.forward_3_backbone(x)
          x = self.forward_4_postprocess(x)
          return x

      ################################################################
      ## Forward related functions END ###############################
      ################################################################

      ################################################################
      ## Init related functions BEGIN ################################
      ################################################################

      def init_setup_stat_parameters_as_float(self):
          self.mean = (
              0.48145466,
              0.4578275,
              0.40821073,
          )
          self.std = (
              0.26862954,
              0.26130258,
              0.27577711,
          )

      def init_timm_model(self):
          self.timm_model = timm.create_model(
              "eva02_large_patch14_448.mim_m38m_ft_in22k_in1k",
              num_classes=3,
              pretrained=True,
          )

      def __init__(self):
          super().__init__()
          self.init_setup_stat_parameters_as_float()
          self.init_timm_model()

      ################################################################
      ## Init related functions END ##################################
      ################################################################
#+end_src

**** function to produce the export
#+begin_src python :shebang #!/usr/bin/env python3 :results output :mkdirp yes :tangle ./main.function.py
  def produce_model(path_file_out):
      with torch.no_grad():
          model = model_wrapper()
          model = torch.compile(
              model=model,
              fullgraph=True,
              dynamic=True,
              backend="inductor",
              mode="max-autotune",
          )
          x = torch.rand(
              (BATCH_SIZE, INPUT_SIZE),
              dtype=torch.float32,
          )
          y = model(x)

          dynamic_shapes = {
              "x": (Dim.DYNAMIC, Dim.STATIC),
          }
          exported_module = torch.export.export(
              model._orig_mod,
              (x,),
              dynamic_shapes=dynamic_shapes,
              # strict=True,
              # dynamic_shapes=dynamic_shapes,
          )

          torch.export.save(
              ep=exported_module,
              f=path_file_out + ".pt2",
          )
#+end_src

**** execute
#+begin_src python :shebang #!/usr/bin/env python3 :results output :mkdirp yes :tangle ./main.execute.py
  produce_model(path_file_out="model_input")
#+end_src

** Compile the model

*** Add the file to git
#+begin_src sh :shebang #!/bin/sh :results output :mkdirp yes :tangle ./.git.sh
  PREPARE_PYTHON_FILE 'compile'
#+end_src

*** Actual file

**** config basic variables
#+begin_src python :shebang #!/usr/bin/env python3 :results output :mkdirp yes :tangle ./compile.config.py
  import os
  import sys
#+end_src

**** import important libraries
#+begin_src python :shebang #!/usr/bin/env python3 :results output :mkdirp yes :tangle ./compile.import.py
  import torch
#+end_src

**** function to compile the export program using AOTinductor
#+begin_src python :shebang #!/usr/bin/env python3 :results output :mkdirp yes :tangle ./compile.function.py
  def compile_to_dynamo(path_file_in, path_file_out):
      exported_module = torch.export.load(f=path_file_in)
      output_path = torch._inductor.aoti_compile_and_package(
          exported_module,
          package_path=path_file_out,
      )
#+end_src

**** execute the compiling function
#+begin_src python :shebang #!/usr/bin/env python3 :results output :mkdirp yes :tangle ./compile.execute.py
  compile_to_dynamo(
      path_file_in=sys.argv[1],
      path_file_out=sys.argv[2],
  )
#+end_src

* C++ program

** Include all the main headers

*** Add the file to git
#+begin_src sh :shebang #!/bin/sh :results output :mkdirp yes :tangle ./.git.sh
  GITADD 'src/main.hpp'
#+end_src

*** Actual file
#+begin_src c++ :mkdirp yes :tangle ./src/main.hpp
  #include <torch/csrc/inductor/aoti_package/model_package_loader.h>
  #include <torch/torch.h>
  #include <vector>
#+end_src

** Main C++ code

*** Add the file to git
#+begin_src sh :shebang #!/bin/sh :results output :mkdirp yes :tangle ./.git.sh
  GITADD 'src/main.cpp'
#+end_src

*** Actual file
#+begin_src c++ :mkdirp yes :tangle ./src/main.cpp
  #include "./all.hpp"

  extern "C" {

  unsigned long constexpr INPUT_SIZE = 100;
  unsigned long constexpr OUTPUT_SIZE = 4;

  struct arg_input {
    float val[INPUT_SIZE];
  };

  struct arg_output {
    float val[OUTPUT_SIZE];
  };

  void do_infer(arg_input const *in, unsigned int batch_size, arg_output *out) {
    static c10::InferenceMode mode;
    static torch::inductor::AOTIModelPackageLoader loader("model_output.pt2");

    std::vector<torch::Tensor> inputs = {
        torch::zeros({batch_size, INPUT_SIZE}, at::kCPU)};

    for (int j = 0; j < batch_size; ++j) {
      for (int i = 0; i < INPUT_SIZE; ++i) {
        inputs[0][j][i] = in[j].val[i];
      }
    }

    std::vector<torch::Tensor> outputs = loader.run(inputs);

    for (int j = 0; j < batch_size; ++j) {
      for (int i = 0; i < OUTPUT_SIZE; i++) {
        out[j].val[i] = outputs[0][j][i].item<float>();
      }
    }
  }
  }
#+end_src

** Header file

*** Add the file to git
#+begin_src sh :shebang #!/bin/sh :results output :mkdirp yes :tangle ./.git.sh
  GITADD 'src/main.h'
#+end_src

*** Actual file
#+begin_src c++ :mkdirp yes :tangle ./src/main.h
  extern "C" {

  unsigned long constexpr INPUT_SIZE = 100;
  unsigned long constexpr OUTPUT_SIZE = 4;

  struct arg_input {
    float val[INPUT_SIZE];
  };

  struct arg_output {
    float val[OUTPUT_SIZE];
  };

  void do_infer(arg_input const *in, unsigned int batch_size, arg_output *out) ;
  }
#+end_src

** Makefile to compile the c++ side

*** Add the file to git
#+begin_src sh :shebang #!/bin/sh :results output :mkdirp yes :tangle ./.git.sh
  GITADD 'Makefile'
#+end_src

*** Actual file
#+begin_src make :tangle ./Makefile
  CC=clang++

  install: build/libmytorch.so model_output.pt2
  	cp -vf -- build/libmytorch.so /lib/

  src/all.hpp: src/main.hpp
  	$(CC) src/main.hpp -o src/all.hpp -E -I/usr/include/torch/csrc/api/include/

  build/main.o: src/main.cpp src/all.hpp
  	mkdir -pv -- ./build
  	$(CC) src/main.cpp -fPIC -c -o build/main.o -O3 -march=x86-64-v3 -mtune=native

  build/libmytorch.so: build/main.o
  	$(CC) build/main.o -o build/libmytorch.so -fPIC -shared -ltorch -ltorch_cpu -flto

  clean:
  	rm -rf -- build src/all.hpp target

  model_output.pt2: model_input.pt2 compile.py
  	./compile.py model_input.pt2 model_output.pt2

  model_input.pt2: main.py
  	./main.py
#+end_src

** Script to compile

*** Add the file to git
#+begin_src sh :shebang #!/bin/sh :results output :mkdirp yes :tangle ./.git.sh
  GITADD 'compile_g++.sh'
#+end_src

*** Actual file
#+begin_src sh :shebang #!/bin/sh :results output :mkdirp yes :tangle ./compile_g++.sh
  mkdir -pv -- './lib/' './tmp/'

  bear -- \
      clang++ \
      './src/main.cpp' -o './tmp/main.o' \
      -c -fPIC \
      '-I/usr/include/torch/csrc/api/include/' \
  ;

  # clang++ \
  #     './tmp.cpp' -o './tmp.exe' \
  #     './tmp/main.o' \
  #     -ltorch \
  #     -ltorch_cpu \
  #     -lc10 \
  # ;
      # -laoti_custom_ops \
      # -lbackend_with_compiler \
      # -lgomp-98b21ff3 \
      # -ljitbackend_test \
      # -lnnapi_backend \
      # -lshm \
      # -ltorch_global_deps \
      # -ltorch_python \
      # -ltorchbind_test \
#+end_src

** compile_commands for lsp

*** Add the file to git
#+begin_src sh :shebang #!/bin/sh :results output :mkdirp yes :tangle ./.git.sh
  GITADD 'compile_commands.json'
#+end_src

*** Actual file
#+begin_src conf :mkdirp yes :tangle ./compile_commands.json
  [
    {
      "arguments": [
        "/usr/bin/clang++",
        "-c",
        "-fPIC",
        "-I/usr/include/torch/csrc/api/include/",
        "-o",
        "./tmp/main.o",
        "./src/main.cpp"
      ],
      "directory": "/data",
      "file": "/data/src/main.cpp",
      "output": "/data/tmp/main.o"
    }
  ]
#+end_src

* Cargo.toml

** Add the file to git
#+begin_src sh :shebang #!/bin/sh :results output :mkdirp yes :tangle ./.git.sh
  GITADD 'Cargo.toml'
#+end_src

** Actual file

*** Main metadata
#+begin_src toml :mkdirp yes :tangle ./Cargo.toml
  [package]
  name = "burn_test"
  version = "0.1.0"
  edition = "2024"

  include = [
      "src/main.cpp",
      "src/main.hpp",
      "src/main.h",
      "src/main.rs",
      "Cargo.toml",
      "Makefile",
      "build.rs",
      "compile.py",
  ]
#+end_src

*** List of files to compile
#+begin_src toml :mkdirp yes :tangle ./Cargo.toml
  [[bin]]
  name = "infer-server"
  path = "src/main.rs"

  [dependencies]
  image = { version = "0.25.6", features = ["serde", "nasm"] }
#+end_src

*** COMMENT Dependencies
#+begin_src toml :mkdirp yes :tangle ./Cargo.toml
  [[bin]]
  name = "infer-server"
  path = "src/main.rs"

  [dependencies]
  actix-multipart = "0.7.2"
  actix-web = "4.11.0"
  bincode = { version = "2.0.1", features = ["serde"] }
  env_logger = "0.11.8"
  futures = "0.3.31"
  futures-util = "0.3.31"
  gxhash = "3.5.0"
  image = { version = "0.25.6", features = ["serde", "nasm"] }
  lockfree = "0.5.1"
  log = "0.4.27"
  ndarray = { version = "0.16.1", features = ["blas", "matrixmultiply-threading", "rayon", "serde"] }
  prost = "0.14"
  serde = { version = "1.0.219", features = ["derive"] }
  thiserror = "2.0.15"
  tokio = { version = "1.47.1", features = ["full"] }
  tonic-prost = "*"
  tonic = { version = "0.14.2", features = ["zstd"] }
#+end_src

* build.rs

** Add the file to git
#+begin_src sh :shebang #!/bin/sh :results output :mkdirp yes :tangle ./.git.sh
  GITADD 'build.rs'
#+end_src

** Actual file
#+begin_src rust :mkdirp yes :tangle ./build.rs
  fn main() {
      println!("cargo:rustc-link-arg=-lmytorch");
  }
#+end_src

* Main rust code

** Add files to git
#+begin_src sh :shebang #!/bin/sh :results output :mkdirp yes :tangle ./.git.sh
  GITADD 'src/main.rs'
#+end_src

** Actual code

*** lib file
#+begin_src rust :mkdirp yes :tangle ./src/mylib.rs
  use image::DynamicImage;
  use image::imageops;

  pub struct image_processor {
      image_resolution: u32,
  }

  impl image_processor {
      pub fn new(val: u32) -> Self {
          return image_processor {
              image_resolution: val,
          };
      }
      fn preprocess(&self, img: DynamicImage) -> image::RgbaImage {
          let (width, height) = (img.width(), img.height());
          let size = width.min(height);
          let x = (width - size) / 2;
          let y = (height - size) / 2;
          let cropped_img = imageops::crop_imm(&img, x, y, size, size).to_image();
          imageops::resize(
              &cropped_img,
              self.image_resolution,
              self.image_resolution,
              imageops::FilterType::CatmullRom,
          )
      }
      pub fn decode_and_preprocess(&self, data: Vec<u8>) -> Result<image::RgbaImage, String> {
          match image::load_from_memory(&data) {
              Ok(img) => {
                  return Ok(self.preprocess(img));
              }
              Err(e) => {
                  return Err("decode error".to_string());
              }
          };
      }
  }
#+end_src

*** Main model code

**** Outputs from bindgen
#+begin_src rust :mkdirp yes :tangle ./src/model.rs
  pub const INPUT_SIZE: ::std::os::raw::c_ulong = 100;
  pub const OUTPUT_SIZE: ::std::os::raw::c_ulong = 4;
  #[repr(C)]
  #[derive(Debug, Copy, Clone)]
  pub struct arg_input {
      pub val: [f32; 100usize],
  }
  #[allow(clippy::unnecessary_operation, clippy::identity_op)]
  const _: () = {
      ["Size of arg_input"][::std::mem::size_of::<arg_input>() - 400usize];
      ["Alignment of arg_input"][::std::mem::align_of::<arg_input>() - 4usize];
      ["Offset of field: arg_input::val"][::std::mem::offset_of!(arg_input, val) - 0usize];
  };
  #[repr(C)]
  #[derive(Debug, Copy, Clone)]
  pub struct arg_output {
      pub val: [f32; 4usize],
  }
  #[allow(clippy::unnecessary_operation, clippy::identity_op)]
  const _: () = {
      ["Size of arg_output"][::std::mem::size_of::<arg_output>() - 16usize];
      ["Alignment of arg_output"][::std::mem::align_of::<arg_output>() - 4usize];
      ["Offset of field: arg_output::val"][::std::mem::offset_of!(arg_output, val) - 0usize];
  };
  unsafe extern "C" {
      pub fn do_infer(
          in_: *const arg_input,
          batch_size: ::std::os::raw::c_uint,
          out: *mut arg_output,
      );
  }
#+end_src

**** Main inference function
#+begin_src rust :mkdirp yes :tangle ./src/model.rs
  pub fn run_inference(input: Vec<arg_input>) -> Vec<arg_output> {
      let tmp = arg_output { val: [0.0; 4usize] };

      let mut output = Vec::<arg_output>::with_capacity(input.len());

      for _ in 0..input.len() {
          output.push(tmp);
      }

      unsafe {
          do_infer(input.as_ptr(), input.len() as u32, output.as_mut_ptr());
      }

      output
  }
#+end_src

**** Actual model wrappers
#+begin_src rust :mkdirp yes :tangle ./src/model.rs
  use serde::Serialize;

  use std::ops::Index;
  use std::time::Duration;

  use ndarray::Array;
  use ndarray::Axis;
  use ndarray::Ix4;

  use tokio;
  use tokio::sync::mpsc;
  use tokio::sync::oneshot;

  use crate::mylib::image_processor;

  type outtype = f32;

  const MAX_BATCH: usize = 16;
  const BATCH_TIMEOUT: Duration = Duration::from_millis(200);
  const IMAGE_RESOLUTION: u32 = 448;
  const CLASS_LABELS: [&str; OUTPUT_SIZE] = ["empty", "occupied", "other", "more"];

  pub struct prediction_probabilities {
      pub ps: [outtype; OUTPUT_SIZE],
  }

  impl prediction_probabilities {
      pub fn new() -> Self {
          prediction_probabilities {
              ps: [0.0; OUTPUT_SIZE],
          }
      }
      pub fn from<T: Index<usize, Output = outtype>>(input: T) -> Self {
          let mut ret = prediction_probabilities::new();
          for i in 0..OUTPUT_SIZE {
              ret.ps[i] = input[i];
          }
          return ret;
      }
  }

  #[derive(Serialize)]
  pub struct prediction_probabilities_reply {
      ps: [String; OUTPUT_SIZE],
      mj: String,
  }

  impl prediction_probabilities_reply {
      pub fn new() -> Self {
          prediction_probabilities_reply {
              ps: std::array::from_fn(|_| String::new()),
              mj: String::new(),
          }
      }
      pub fn from(input: prediction_probabilities) -> prediction_probabilities_reply {
          let mut max_index: usize = 0;
          let mut ret = prediction_probabilities_reply::new();
          ret.ps[0] = input.ps[0].to_string();
          for i in 1..OUTPUT_SIZE {
              ret.ps[i] = input.ps[i].to_string();
              if input.ps[i] > input.ps[max_index] {
                  max_index = i;
              }
          }
          ret.mj = CLASS_LABELS[max_index].to_string();
          return ret;
      }
  }

  pub struct InferRequest {
      img: image::RgbaImage,
      resp_tx: oneshot::Sender<Result<prediction_probabilities, String>>,
  }

  pub struct model_server {
      rx: mpsc::Receiver<InferRequest>,
      session: Session,
  }

  impl model_server {
      pub async fn infer_loop(&mut self) {
          while let Some(first) = self.rx.recv().await {
              let mut batch = vec![first];
              let start = tokio::time::Instant::now();
              while batch.len() < MAX_BATCH && start.elapsed() < BATCH_TIMEOUT {
                  match self.rx.try_recv() {
                      Ok(req) => batch.push(req),
                      Err(_) => break,
                  }
              }
              let batch_size = batch.len();
              let mut input = Array::<u8, Ix4>::zeros((
                  batch_size,
                  IMAGE_RESOLUTION as usize,
                  IMAGE_RESOLUTION as usize,
                  3,
              ));
              for (i, req) in batch.iter().enumerate() {
                  for (x, y, pixel) in req.img.enumerate_pixels() {
                      let [r, g, b, _] = pixel.0;
                      input[[i, y as usize, x as usize, 0]] = r;
                      input[[i, y as usize, x as usize, 1]] = g;
                      input[[i, y as usize, x as usize, 2]] = b;
                  }
              }
              let outputs = match self
                  .session
                  .run(inputs!["input" => TensorRef::from_array_view(&input).unwrap()])
              {
                  Ok(o) => o,
                  Err(e) => {
                      for req in batch {
                          let _ = req.resp_tx.send(Err(format!("inference error: {}", e)));
                      }
                      continue;
                  }
              };
              let output = outputs["output"]
                  .try_extract_array::<outtype>()
                  .unwrap()
                  .t()
                  .into_owned();
              for (row, req) in output.axis_iter(Axis(1)).zip(batch.into_iter()) {
                  let result = prediction_probabilities::from(row);
                  let _ = req.resp_tx.send(Ok(result));
              }
          }
      }
  }

  pub struct model_client {
      tx: mpsc::Sender<InferRequest>,
      preprocess: image_processor,
  }

  impl model_client {
      pub async fn do_infer(
          &self,
          img: image::RgbaImage,
      ) -> Result<prediction_probabilities, String> {
          let (resp_tx, resp_rx) = oneshot::channel();
          match self.tx.send(InferRequest { img, resp_tx }).await {
              Ok(_) => match resp_rx.await {
                  Ok(Ok(pred)) => {
                      return Ok(pred);
                  }
                  Ok(Err(e)) => {
                      return Err(e);
                  }
                  Err(e) => {
                      return Err("Recv Error".to_string());
                  }
              },
              Err(e) => {
                  return Err("Send error".to_string());
              }
          }
      }
      pub async fn do_infer_data(&self, data: Vec<u8>) -> Result<prediction_probabilities, String> {
          match self.preprocess.decode_and_preprocess(data) {
              Ok(img) => {
                  return self.do_infer(img).await;
              }
              Err(e) => {
                  return Err("Failed to decode and pre-process the image".to_string());
              }
          }
      }
  }

  pub fn get_inference_tuple() -> (model_server, model_client) {
      let (tx, rx) = mpsc::channel::<InferRequest>(512);
      let ret_server = model_server {
          rx: rx,
          session: get_model(MODEL_PATH),
      };
      let ret_client = model_client {
          tx: tx,
          preprocess: image_processor::new(IMAGE_RESOLUTION),
      };
      return (ret_server, ret_client);
  }
#+end_src

**** COMMENT Remaining wrappers
#+begin_src rust :mkdirp yes :tangle ./src/model.rs
#+end_src

*** Main code
#+begin_src rust :mkdirp yes :tangle ./src/main.rs
  mod model;
  mod mylib;
  fn main() {
      let mut input = crate::model::arg_input {
          val: [0.0; 100usize],
      };

      for i in 0..100 {
          input.val[i] = (i as f32) / 100.0;
      }

      let vec_input = vec![input, input, input];
      let vec_output = crate::model::run_inference(vec_input);
      println!("{:?}", vec_output);
  }
#+end_src

* Script to run the rust code

** Add the file to git
#+begin_src sh :shebang #!/bin/sh :results output :mkdirp yes :tangle ./.git.sh
  GITADD 'run.sh'
#+end_src

** Actual file
#+begin_src sh :shebang #!/bin/sh :results output :mkdirp yes :tangle ./run.sh
  cd "$(dirname -- "${0}")"
  make -j
  export RUSTFLAGS="-C target-cpu=native"
  cargo run --release infer-server
#+end_src

* COMMENT Work space
#+begin_src emacs-lisp :results silent
  (save-buffer)
  (org-babel-tangle)
  (async-shell-command "
      './.git.sh'
  " "log" "err")
#+end_src
