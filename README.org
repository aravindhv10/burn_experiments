* Scripts to manage
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./.git.sh
  . "${HOME}/important_functions.sh"

  CLEAN '#README.org#'
  CLEAN '.git.sh'
  CLEAN 'README.org~'

  GITADD 'README.org'
#+end_src

* Write a curl script for sample execution

** Add file to git
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./.git.sh
  GITADD 'curl.sh'
#+end_src

** Actual file
#+begin_src sh :shebang #!/bin/sh :results output :mkdirp yes :tangle ./curl.sh
  curl -X POST "http://127.0.0.1:8000/infer" -F "file=@./image.png"
  curl -X POST "http://127.0.0.1:8000/infer" -F "file=@./image.jpg"
#+end_src

* Script to build docker image

** Add file to git
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./.git.sh
  GITADD 'build_container.sh'
#+end_src

** Actual script to build the container
#+begin_src sh :shebang #!/bin/sh :results output :mkdirp yes :tangle ./build_container.sh
  cd "$('dirname' -- "${0}")"
  IMAGE_NAME='7_rust_libtorch_demo'
  mkdir -pv -- './build'

  H="$(cat './CMakeLists.txt' './Cargo.toml' './Dockerfile' './build.rs' './compile.py' './compile.sh' './curl.sh' './infer.proto' './infer.sh' './main.py' './run.sh' './src/client.rs' './src/export.hpp' './src/main.cpp' './src/main.hpp' './src/main.rs' './src/model.rs' './src/mylib.rs' './src/test.cpp' | sha512sum | cut -d ' ' -f1)"

  test -e "./build/${H}" && exit '0'
  CMD='sudo -A docker'
  which buildah && CMD='buildah'
  ${CMD} build -t "${IMAGE_NAME}" -f './Dockerfile' . && touch "./build/${H}"
  exit '0'
#+end_src

* Python file to produce the model file and compile it

** Produce the model

*** Add the file to git
#+begin_src sh :shebang #!/bin/sh :results output :mkdirp yes :tangle ./.git.sh
  PREPARE_PYTHON_FILE 'main'
#+end_src

*** Actual file

**** config the import paths
#+begin_src python :shebang #!/usr/bin/env python3 :results output :mkdirp yes :tangle ./main.config.py
  import os

  try:
      __file__
  except:
      basepath = "."
  else:
      basepath = os.path.abspath(os.path.dirname(__file__) + "/")
  import sys

  sys.path.append(basepath)
#+end_src

**** config standard variables
#+begin_src python :shebang #!/usr/bin/env python3 :results output :mkdirp yes :tangle ./main.config.py
  IMAGE_RESOLUTION = 448
  BATCH_SIZE = 4
  NUM_CHANNELS = 3
  NUM_CLASSES = 3

  (
      SIZE_B,
      SIZE_Y,
      SIZE_X,
      SIZE_C,
  ) = (
      BATCH_SIZE,
      IMAGE_RESOLUTION,
      IMAGE_RESOLUTION,
      NUM_CHANNELS,
  )

  INPUT_SHAPE = (
      SIZE_B,
      SIZE_Y,
      SIZE_X,
      SIZE_C,
  )
#+end_src

**** import important libraries
#+begin_src python :shebang #!/usr/bin/env python3 :results output :mkdirp yes :tangle ./main.import.py
  import os
  import sys
  import einops
  import timm
  import torch
  from torch.export.dynamic_shapes import Dim
#+end_src

**** class to define the actual model
#+begin_src python :shebang #!/usr/bin/env python3 :results output :mkdirp yes :tangle ./main.class.py
  class model_wrapper(torch.nn.Module):
      ################################################################
      ## Forward related functions BEGIN #############################
      ################################################################

      def forward_1_rearrange(
          self,
          x: torch.Tensor,
      ):
          x = einops.rearrange(
              x,
              "B Y X C -> B C Y X",
          )
          return x

      def forward_2_normalize(
          self,
          x: torch.Tensor,
      ):
          for i in range(SIZE_C):
              x[:, i, :, :] = ((x[:, i, :, :] / 255.0) - self.mean[i]) / self.std[i]

          return x

      def forward_3_backbone(
          self,
          x: torch.Tensor,
      ):
          x = self.timm_model(x)
          return x

      def forward_4_postprocess(
          self,
          x: torch.Tensor,
      ):
          x = torch.nn.functional.softmax(
              x,
              dim=1,
          )
          return x

      def forward(
          self,
          x: torch.Tensor,
      ):
          x = self.forward_1_rearrange(x)
          x = self.forward_2_normalize(x)
          x = self.forward_3_backbone(x)
          x = self.forward_4_postprocess(x)
          return x

      ################################################################
      ## Forward related functions END ###############################
      ################################################################

      ################################################################
      ## Init related functions BEGIN ################################
      ################################################################

      def init_setup_stat_parameters_as_float(self):
          self.mean = (
              0.48145466,
              0.4578275,
              0.40821073,
          )
          self.std = (
              0.26862954,
              0.26130258,
              0.27577711,
          )

      def init_timm_model(self):
          self.timm_model = timm.create_model(
              "timm/eva02_base_patch14_448.mim_in22k_ft_in1k",
              num_classes=NUM_CLASSES,
              pretrained=True,
          )

      def __init__(self):
          super().__init__()
          self.init_setup_stat_parameters_as_float()
          self.init_timm_model()

      ################################################################
      ## Init related functions END ##################################
      ################################################################
#+end_src

**** function to produce the export
#+begin_src python :shebang #!/usr/bin/env python3 :results output :mkdirp yes :tangle ./main.function.py
  def produce_model(path_file_out):
      dtype = torch.bfloat16
      model = model_wrapper()
      model.eval()
      with torch.inference_mode():
          inductor_configs = {}
          if torch.cuda.is_available():
              device = "cuda"
              inductor_configs["max_autotune"] = True
          else:
              device = "cpu"

          print("device = ", device)
          print("dtype = ", dtype)

          model = model.to(device=device, dtype=dtype)
          x = torch.rand(
              INPUT_SHAPE,
              dtype=dtype,
              device=device,
          )

          dynamic_shapes = {
              "x": (
                  Dim.DYNAMIC,
                  Dim.STATIC,
                  Dim.STATIC,
                  Dim.STATIC,
              ),
          }

          exported_program = torch.export.export(
              # model._orig_mod,
              model,
              (x,),
              dynamic_shapes=dynamic_shapes,
              strict=True,
          )

          path = torch._inductor.aoti_compile_and_package(
              exported_program,
              package_path=path_file_out,
              inductor_configs=inductor_configs,
          )
#+end_src

**** function to test if it works
#+begin_src python :shebang #!/usr/bin/env python3 :results output :mkdirp yes :tangle ./main.function.py
  def test_model(path_file_in):
      dtype=torch.bfloat16
      if torch.cuda.is_available():
          device = "cuda"
      else:
          device = "cpu"
      model = torch._inductor.aoti_load_package(path_file_in)
      x = torch.rand(
          (
              SIZE_B * 2,
              SIZE_Y,
              SIZE_X,
              SIZE_C,
          ),
          dtype=dtype,
          device=device,
      )
      with torch.inference_mode():
          output = model(x)

          print(output)
      return output
#+end_src

**** Function wrapper to maintain compatibility
#+begin_src python :shebang #!/usr/bin/env python3 :results output :mkdirp yes :tangle ./main.function.py
  def compile_model_to_aot_inductor(path_file_input, path_file_output):
      produce_model(path_file_out=path_file_output)
#+end_src

**** COMMENT execute
#+begin_src python :shebang #!/usr/bin/env python3 :results output :mkdirp yes :tangle ./main.execute.py
  produce_model(path_file_out=sys.argv[1])
  test_model(path_file_in=sys.argv[1])
#+end_src

* Python program to export the model

** Add file to git
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./.git.sh
  GITADD 'compile.py'
#+end_src

** Actual file
#+begin_src python :shebang #!/usr/bin/env python3 :results output :mkdirp yes :tangle ./compile.py
  from main import compile_model_to_aot_inductor
  import sys

  if __name__ == "__main__":
      compile_model_to_aot_inductor(sys.argv[1], sys.argv[2])
#+end_src

* Dockerfile to produce single self contained image

** Add docker file to git
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./.git.sh
  GITADD 'Dockerfile'
#+end_src

** Actual docker file
#+begin_src conf :mkdirp yes :tangle ./Dockerfile
  FROM 6_pytorch

  RUN mkdir -pv -- '/root/source' '/root/build'

  COPY './CMakeLists.txt' '/root/source/CMakeLists.txt'
  COPY './Cargo.toml' '/root/source/Cargo.toml'
  COPY './build.rs' '/root/source/build.rs'
  COPY './compile.py' '/root/source/compile.py'
  COPY './compile.sh' '/root/source/compile.sh'
  COPY './curl.sh' '/root/curl.sh'
  COPY './infer.proto' '/root/source/infer.proto'
  COPY './infer.sh' '/root/infer.sh'
  COPY './main.py' '/root/source/main.py'
  COPY './run.sh' '/root/run.sh'
  COPY './src/client.rs' '/root/source/src/client.rs'
  COPY './src/export.hpp' '/root/source/src/export.hpp'
  COPY './src/main.cpp' '/root/source/src/main.cpp'
  COPY './src/main.hpp' '/root/source/src/main.hpp'
  COPY './src/main.rs' '/root/source/src/main.rs'
  COPY './src/model.rs' '/root/source/src/model.rs'
  COPY './src/mylib.rs' '/root/source/src/mylib.rs'
  COPY './src/test.cpp' '/root/source/src/test.cpp'

  RUN \
      echo 'START Compile c++ parts' \
      && cd '/root/build' \
      && cmake '../source' \
      && make -j4 \
      && make install \
      && echo 'DONE Compile c++ parts' ;


  ENV RUSTFLAGS="-C target-cpu=native"
  ENV CARGO_TARGET_DIR="/root/build/cargo"

  RUN \
      echo 'START Compiling rust parts' \
      && cd '/root/source' \
      && bindgen './src/export.hpp' > './src/export.rs' \
      && cargo build --bin infer-server --release \
      && cargo build --bin infer-client --release \
      && install --compare  "${CARGO_TARGET_DIR}/release/infer-server" '/usr/bin/infer-server' \
      && install --compare  "${CARGO_TARGET_DIR}/release/infer-client" '/usr/bin/infer-client' \
      && echo 'DONE Compiling rust parts' ;
#+end_src

* Scripts to run image

** Add file to git
#+begin_src sh :shebang #!/bin/sh :results output :mkdirp yes :tangle ./.git.sh
  GITADD 'image_run.sh'
  GITADD 'image_run_amd.sh'
  GITADD 'image_run_nvidia.sh'
#+end_src

** Actual script

*** For cpu
#+begin_src sh :shebang #!/bin/sh :results output :mkdirp yes :tangle ./image_run.sh
  cd "$(dirname -- "${0}")"

  mkdir -pv -- "${HOME}/BUILD"

  IMAGE_NAME='7_rust_libtorch_demo'

  RUN_CONTAINER () {
      CMD='sudo -A docker'
      which podman && CMD='podman'
      ${CMD} run \
          -it --rm \
          '--security-opt' 'seccomp=unconfined' \
          --mount 'type=tmpfs,destination=/data/TMPFS,tmpfs-size=137438953472' \
          -v "$(realpath .):/data/source" \
          -v "${HOME}/BUILD:/data/build" \
          -v "CACHE:/usr/local/cargo/registry" \
          -v "CACHE:/root/.cache" \
          "${IMAGE_NAME}" zsh \
      ;
  }

  RUN_CONTAINER
#+end_src

*** For amd gpu
#+begin_src sh :shebang #!/bin/sh :results output :mkdirp yes :tangle ./image_run_amd.sh
  cd "$(dirname -- "${0}")"

  mkdir -pv -- "${HOME}/BUILD"

  IMAGE_NAME='7_rust_libtorch'

  RUN_CONTAINER () {
      CMD='sudo -A docker'
      which podman && CMD='podman'
      ${CMD} run \
          -it --rm \
          '--device' '/dev/kfd' \
          '--device' '/dev/dri' \
          '--security-opt' 'seccomp=unconfined' \
          --mount 'type=tmpfs,destination=/data/TMPFS,tmpfs-size=137438953472' \
          -v "$(realpath .):/data/source" \
          -v "${HOME}/BUILD:/data/build" \
          -v "CACHE:/usr/local/cargo/registry" \
          -v "CACHE:/root/.cache" \
          -e 'ROCR_VISIBLE_DEVICES=0' \
          "${IMAGE_NAME}" zsh \
      ;
  }

  RUN_CONTAINER
#+end_src

*** For nvidia gpu
#+begin_src sh :shebang #!/bin/sh :results output :mkdirp yes :tangle ./image_run_nvidia.sh
  cd "$(dirname -- "${0}")"

  mkdir -pv -- "${HOME}/BUILD"

  IMAGE_NAME='7_rust_libtorch'

  RUN_CONTAINER () {
      CMD='sudo -A docker'
      which podman && CMD='podman'
      ${CMD} run \
          --tty \
          --interactive \
          --rm \
          --gpus 'all,"capabilities=compute,utility,video"' \
          --ipc host \
          --ulimit memlock=-1 \
          --ulimit stack=67108864 \
          --shm-size 107374182400 \
          --mount 'type=tmpfs,destination=/data/TMPFS,tmpfs-size=137438953472' \
          -v "$(realpath .):/data/source" \
          -v "${HOME}/BUILD:/data/build" \
          -v "CACHE:/usr/local/cargo/registry" \
          -v "CACHE:/root/.cache" \
          -v "CACHE:/root/.triton" \
          "${IMAGE_NAME}" zsh \
      ;
  }

  RUN_CONTAINER
#+end_src

* Main c++ - rust interface

** Add the file to git
#+begin_src sh :shebang #!/bin/sh :results output :mkdirp yes :tangle ./.git.sh
  GITADD 'src/export.hpp'
  GITADD 'src/export.rs'
#+end_src

** Actual interface headers

*** C++ header
#+begin_src c++ :mkdirp yes :tangle ./src/export.hpp
  extern "C" {

  unsigned int constexpr IMAGE_RESOLUTION = 448;
  unsigned int constexpr NUM_CHANNELS = 3;
  unsigned int constexpr NUM_CLASSES = 3;

  unsigned int constexpr SIZE_Y = IMAGE_RESOLUTION;
  unsigned int constexpr SIZE_X = IMAGE_RESOLUTION;
  unsigned int constexpr SIZE_C = NUM_CHANNELS;
  unsigned int constexpr SIZE_O = NUM_CLASSES;

  typedef unsigned char intype;
  typedef float outtype;

  struct arg_input {
    intype val[SIZE_Y][SIZE_X][SIZE_C];
  };

  struct arg_output {
    outtype val[SIZE_O];
  };

  void mylibtorchinfer(arg_input *in, unsigned int const batch_size, arg_output *out);
  bool decode_image_data(unsigned char *binary_data, int data_size, arg_input * dst_struct);
  }
#+end_src

*** rs header
#+begin_src c++ :mkdirp yes :tangle ./src/export.rs
  /* automatically generated by rust-bindgen 0.72.0 */

  pub const IMAGE_RESOLUTION: ::std::os::raw::c_uint = 448;
  pub const NUM_CHANNELS: ::std::os::raw::c_uint = 3;
  pub const NUM_CLASSES: ::std::os::raw::c_uint = 3;
  pub const SIZE_Y: ::std::os::raw::c_uint = 448;
  pub const SIZE_X: ::std::os::raw::c_uint = 448;
  pub const SIZE_C: ::std::os::raw::c_uint = 3;
  pub const SIZE_O: ::std::os::raw::c_uint = 3;
  pub type intype = u8;
  pub type outtype = f32;
  #[repr(C)]
  #[derive(Debug, Copy, Clone)]
  pub struct arg_input {
      pub val: [[[intype; 3usize]; 448usize]; 448usize],
  }
  #[allow(clippy::unnecessary_operation, clippy::identity_op)]
  const _: () = {
      ["Size of arg_input"][::std::mem::size_of::<arg_input>() - 2408448usize];
      ["Alignment of arg_input"][::std::mem::align_of::<arg_input>() - 4usize];
      ["Offset of field: arg_input::val"][::std::mem::offset_of!(arg_input, val) - 0usize];
  };
  #[repr(C)]
  #[derive(Debug, Copy, Clone)]
  pub struct arg_output {
      pub val: [outtype; 3usize],
  }
  #[allow(clippy::unnecessary_operation, clippy::identity_op)]
  const _: () = {
      ["Size of arg_output"][::std::mem::size_of::<arg_output>() - 12usize];
      ["Alignment of arg_output"][::std::mem::align_of::<arg_output>() - 4usize];
      ["Offset of field: arg_output::val"][::std::mem::offset_of!(arg_output, val) - 0usize];
  };
  unsafe extern "C" {
      pub fn mylibtorchinfer(
          in_: *mut arg_input,
          batch_size: ::std::os::raw::c_uint,
          out: *mut arg_output,
      );
  }
  unsafe extern "C" {
      pub fn decode_image_data(
          binary_data: *mut ::std::os::raw::c_uchar,
          data_size: ::std::os::raw::c_int,
          dst_struct: *mut arg_input,
      ) -> bool;
  }
#+end_src

* Main c++ implementation part

** Include all the main headers

*** Add the file to git
#+begin_src sh :shebang #!/bin/sh :results output :mkdirp yes :tangle ./.git.sh
  GITADD 'src/main.hpp'
#+end_src

*** Actual file
#+begin_src c++ :mkdirp yes :tangle ./src/main.hpp
  #include "./export.hpp"
  #include <iostream>
  #include <opencv2/core/mat.hpp>
  #include <opencv2/imgcodecs.hpp>
  #include <opencv2/opencv.hpp>
  #include <torch/csrc/inductor/aoti_package/model_package_loader.h>
  #include <torch/torch.h>
  #include <vector>
#+end_src

** Main C++ code

*** Add the file to git
#+begin_src sh :shebang #!/bin/sh :results output :mkdirp yes :tangle ./.git.sh
  GITADD 'src/main.cpp'
#+end_src

*** Actual file
#+begin_src c++ :mkdirp yes :tangle ./src/main.cpp
  #include "./main.hpp"

  inline cv::Mat process_image_data(unsigned char *binary_data, int data_size) {
    cv::Mat decoded_img = cv::imdecode(cv::Mat(1, data_size, CV_8UC1, binary_data), cv::IMREAD_COLOR);
    if (decoded_img.empty()) {return cv::Mat::zeros(SIZE_Y, SIZE_X, CV_8UC1);}

    int const height = decoded_img.rows;
    int const width = decoded_img.cols;
    int target_height = IMAGE_RESOLUTION;
    int target_width = IMAGE_RESOLUTION;
    int x_start = 0;
    int y_start = 0;
    float ar = 1;

    if (height < width) {
      ar = float(width) / float(height) ;
      target_width = int(float(IMAGE_RESOLUTION) * ar);
      x_start = static_cast<unsigned int>(target_width - IMAGE_RESOLUTION) >> 1;
    } else {
      ar = float(height) / float(width) ;
      target_height = int(float(IMAGE_RESOLUTION) * ar);
      y_start = static_cast<unsigned int>(target_height - IMAGE_RESOLUTION) >> 1;
    }

    cv::Mat resized_img;
    if ((height > IMAGE_RESOLUTION) && (width > IMAGE_RESOLUTION)) {
      cv::resize(decoded_img, resized_img, cv::Size(target_width, target_height), 0, 0, cv::INTER_AREA);
    } else {
      cv::resize(decoded_img, resized_img, cv::Size(target_width, target_height), 0, 0, cv::INTER_LANCZOS4);
    }

    cv::Rect roi(x_start, y_start, IMAGE_RESOLUTION, IMAGE_RESOLUTION);
    cv::Mat cropped_img = resized_img(roi);

    if (false) {
      return cropped_img;
    } else {
      cv::Mat rgb_img;
      cv::cvtColor(cropped_img, rgb_img, cv::COLOR_BGR2RGB);
      return rgb_img;
    }
  }

  inline bool convertMatToStruct(const cv::Mat& src_mat, arg_input& dst_struct) {
      if (src_mat.rows != SIZE_Y || src_mat.cols != SIZE_X) {return false;}
      if (src_mat.type() != CV_8UC3) {return false;}
      if (src_mat.isContinuous()) {
          constexpr size_t EXPECTED_SIZE_BYTES = SIZE_Y * SIZE_X * SIZE_C * sizeof(intype);
          const uint8_t* mat_data_ptr = src_mat.data;
          uint8_t* struct_data_ptr = reinterpret_cast<uint8_t*>(dst_struct.val);
          std::memcpy(struct_data_ptr, mat_data_ptr, EXPECTED_SIZE_BYTES);
      } else {
          constexpr size_t ROW_SIZE_BYTES = SIZE_X * SIZE_C * sizeof(intype);
          if(false) {
              for (int y = 0; y < SIZE_Y; ++y) {
                  const uint8_t* src_row = src_mat.ptr<uint8_t>(y);
                  for (int x = 0; x < SIZE_X; ++x) {
                      for(int c=0; c<SIZE_C; ++c){
                          dst_struct.val[y][x][c] = src_row[(x*SIZE_C) + (SIZE_C-1-c)];
                      }
                  }
              }
          } else {
              constexpr size_t ROW_SIZE_BYTES = SIZE_X * SIZE_C * sizeof(intype);
              for (int y = 0; y < SIZE_Y; ++y) {
                  const uint8_t* src_row = src_mat.ptr<uint8_t>(y);
                  uint8_t* dst_row = reinterpret_cast<uint8_t*>(dst_struct.val[y]);
                  std::memcpy(dst_row, src_row, ROW_SIZE_BYTES);
              }
          }
      }
      return true;
  }

  inline torch::TensorOptions get_good_device_and_dtype(){
      if (torch::cuda::is_available()) {
          return torch::TensorOptions().dtype(torch::kBFloat16).device(torch::kCUDA);
      } else {
          return torch::TensorOptions().dtype(torch::kBFloat16).device(torch::kCPU);
      }
  }

  inline torch::TensorOptions get_host_device_and_dtype(){
      return torch::TensorOptions().dtype(torch::kUInt8).device(torch::kCPU);
  }

  class infer_slave {
    c10::InferenceMode mode;
    torch::inductor::AOTIModelPackageLoader loader;
    torch::TensorOptions options;
    torch::TensorOptions options_host;
    torch::Tensor input_tensor;
    std::vector<torch::Tensor> inputs;
    std::vector<torch::Tensor> outputs;
    torch::Tensor out_tensor;
    std::size_t bytes_to_copy;

  public:
    inline void operator()(arg_input *in, unsigned int const batch_size, arg_output *out) {
      torch::Tensor cpu_tensor = torch::from_blob(static_cast<void *>(in), {batch_size, SIZE_Y, SIZE_X, SIZE_C}, options_host);
      inputs[0] = cpu_tensor.to(options);
      outputs = loader.run(inputs);
      out_tensor = outputs[0].contiguous().cpu().to(options_host);
      bytes_to_copy = batch_size * SIZE_O * sizeof(outtype);
      std::memcpy(out, out_tensor.data_ptr<outtype>(), bytes_to_copy);
    }

    infer_slave()
        : loader("/model.pt2"),
          options(get_good_device_and_dtype()),
          options_host(get_host_device_and_dtype()) {
      inputs.resize(1);
    }


    ~infer_slave() {}
  };

  infer_slave slave;


  extern "C" {
    void mylibtorchinfer(arg_input *in, unsigned int const batch_size, arg_output *out) {slave(in,batch_size,out);}
    bool decode_image_data(unsigned char *binary_data, int data_size, arg_input * dst_struct){
      /*inline*/ cv::Mat ret = process_image_data(/*unsigned char *binary_data =*/ binary_data, /*int data_size =*/ data_size) ;
      /*inline*/ bool res = convertMatToStruct(/*const cv::Mat& src_mat =*/ ret, /*arg_input& dst_struct =*/ *dst_struct) ;
      return res;
    }
  }
#+end_src

** Include main function to test

*** Add the file to git
#+begin_src sh :shebang #!/bin/sh :results output :mkdirp yes :tangle ./.git.sh
  GITADD './src/test.cpp'
#+end_src

*** Call the function
#+begin_src c++ :mkdirp yes :tangle ./src/test.cpp
  #include "./main.cpp"
  int main(){
    arg_input in;
    arg_output out;

    for(int y = 0 ; y < IMAGE_RESOLUTION; ++y){
      for(int x=0; x<IMAGE_RESOLUTION; ++x){
        for(int c=0; c<NUM_CHANNELS; ++c){
          in.val[y][x][c] = 0;
        }
      }
    }

    mylibtorchinfer(/*arg_input *in =*/ &in, /*unsigned int const batch_size =*/ 1, /*arg_output *out =*/ &out);
    for(int i = 0; i< NUM_CLASSES; ++i){printf("%lf, ",out.val[i]);}
    printf("\n");
    
    return 0;
  }
#+end_src

** CMake parts

*** Add the file to git
#+begin_src sh :shebang #!/bin/sh :results output :mkdirp yes :tangle ./.git.sh
  GITADD 'CMakeLists.txt'
#+end_src

*** Actual file
#+begin_src conf :mkdirp yes :tangle ./CMakeLists.txt
  cmake_minimum_required(VERSION 3.18 FATAL_ERROR)

  project(aoti_example VERSION 1.0 DESCRIPTION "Interface for inferring with libtorch exposed to rust")

  find_package(Torch REQUIRED)
  find_package(OpenCV REQUIRED)

  add_executable(aoti_example src/test.cpp)
  add_library(mytorch SHARED src/main.cpp)

  target_link_libraries(aoti_example "${TORCH_LIBRARIES}" "${OpenCV_LIBS}")
  target_link_libraries(mytorch "${TORCH_LIBRARIES}" "${OpenCV_LIBS}")

  set_property(TARGET aoti_example PROPERTY CXX_STANDARD 17)
  set_target_properties(mytorch PROPERTIES VERSION ${PROJECT_VERSION} CXX_STANDARD 17)

  install(TARGETS mytorch LIBRARY DESTINATION lib)

  set(CMAKE_C_FLAGS "${CMAKE_C_FLAGS} -O3 -mtune=native -march=native -flto")
  set(CMAKE_CXX_FLAGS "${CMAKE_C_FLAGS} -O3 -mtune=native -march=native -flto")
  set(CMAKE_INSTALL_PREFIX "/usr")
  set(CMAKE_EXPORT_COMPILE_COMMANDS 1)
#+end_src

* Cargo.toml

** Add the file to git
#+begin_src sh :shebang #!/bin/sh :results output :mkdirp yes :tangle ./.git.sh
  GITADD 'Cargo.toml'
#+end_src

** Actual file

*** Main metadata
#+begin_src toml :mkdirp yes :tangle ./Cargo.toml
  [package]
  name = "burn_test"
  version = "0.1.0"
  edition = "2024"

  include = [
      "src/main.cpp",
      "src/main.hpp",
      "src/main.rs",
      "Cargo.toml",
      "build.rs",
      "compile.py",
  ]
#+end_src

*** List of files to compile

**** Server
#+begin_src toml :mkdirp yes :tangle ./Cargo.toml
  [[bin]]
  name = "infer-server"
  path = "src/main.rs"
#+end_src

**** GRPC Client
#+begin_src toml :mkdirp yes :tangle ./Cargo.toml
  [[bin]]
  name = "infer-client"
  path = "src/client.rs"
#+end_src

*** List of build dependencies
#+begin_src toml :mkdirp yes :tangle ./Cargo.toml
  [build-dependencies]
  tonic-prost-build = "0.14.2"
#+end_src

*** List of dependencies
#+begin_src toml :mkdirp yes :tangle ./Cargo.toml
  [dependencies]
  actix-multipart = "0.7.2"
  actix-web = "4.11.0"
  futures-util = "0.3.31"
  image = { version = "0.25.6", features = ["serde", "nasm"] }
  prost = "0.14"
  serde = { version = "1.0.219", features = ["derive"] }
  tokio = { version = "1.47.1", features = ["full"] }
  tonic-prost = "*"
  tonic = { version = "0.14.2", features = ["zstd"] }
#+end_src

* proto file

** Add the file
#+begin_src sh :shebang #!/bin/sh :results output :mkdirp yes :tangle ./.git.sh
  GITADD 'infer.proto'
#+end_src

** Actual file
#+begin_src rust :tangle ./infer.proto
  syntax = "proto3";

  package infer;

  message Image {
      bytes image_data = 1;
  }

  message Prediction {
      float ps1 = 1;
      float ps2 = 2;
      float ps3 = 3;
  }

  service Infer {
    rpc doInfer(Image) returns (Prediction) {}
  }
#+end_src

* build.rs

** Add the file to git
#+begin_src sh :shebang #!/bin/sh :results output :mkdirp yes :tangle ./.git.sh
  GITADD 'build.rs'
#+end_src

** Actual file
#+begin_src rust :mkdirp yes :tangle ./build.rs
  fn main() -> Result<(), Box<dyn std::error::Error>> {
      tonic_prost_build::compile_protos("./infer.proto")?;
      println!("cargo:rustc-link-arg=-lmytorch");
      Ok(())
  }
#+end_src

* GRPC inference code

** Add files to git
#+begin_src sh :shebang #!/bin/sh :results output :mkdirp yes :tangle ./.git.sh
  GITADD 'src/client.rs'
#+end_src

** Code for client
#+begin_src rust :mkdirp yes :tangle ./src/client.rs
  pub mod infer {
      tonic::include_proto!("infer"); // The string specified here must match the proto package name
  }

  use std::fs;

  #[tokio::main]
  async fn main() -> Result<(), Box<dyn std::error::Error>> {
      let data = fs::read("./image.jpg").expect("Failed reading image file");
      let img = infer::Image { image_data: data };
      let mut client = infer::infer_client::InferClient::connect("http://127.0.0.1:8001").await?;
      let res = client.do_infer(img).await?;
      println!("{:?}", res);
      return Ok(());
  }
#+end_src

* Main rust code

** Add files to git
#+begin_src sh :shebang #!/bin/sh :results output :mkdirp yes :tangle ./.git.sh
  GITADD 'src/main.rs'
  GITADD 'src/model.rs'
  GITADD 'src/mylib.rs'
#+end_src

** Actual code

*** lib file
#+begin_src rust :mkdirp yes :tangle ./src/mylib.rs
  pub struct image_processor {
      image_resolution: u32,
  }

  impl image_processor {

      pub fn new(val: u32) -> Self {
          return image_processor {
              image_resolution: val,
          };
      }

      fn preprocess(&self, img:  image::DynamicImage) -> image::RgbaImage {
          let (width, height) = (img.width(), img.height());
          let size = width.min(height);
          let x = (width - size) / 2;
          let y = (height - size) / 2;
          let cropped_img = image::imageops::crop_imm(&img, x, y, size, size).to_image();
          image::imageops::resize(
              &cropped_img,
              self.image_resolution,
              self.image_resolution,
              image::imageops::FilterType::CatmullRom,
          )
      }

      pub fn decode_and_preprocess(&self, data: Vec<u8>) -> Result<image::RgbaImage, String> {
          match image::load_from_memory(&data) {
              Ok(img) => {
                  return Ok(self.preprocess(img));
              }
              Err(e) => {
                  return Err("decode error".to_string());
              }
          };
      }
  }
#+end_src

*** Model code

**** Outputs from bindgen
#+begin_src rust :mkdirp yes :tangle ./src/model.rs
  include!("export.rs");
#+end_src

**** Implement methods for input class
#+begin_src rust :mkdirp yes :tangle ./src/model.rs
  impl arg_input {
      pub fn new() -> Self {
          arg_input {
              val: [[[0 as u8; SIZE_C as  usize]; SIZE_X as usize]; SIZE_X as usize],
          }
      }

      pub fn from(mut binary_image_data: Vec<u8>) -> Result<Box<Self>, Box<Self>>  {
          let mut tmp: Box<std::mem::MaybeUninit<Self>> = Box::new_uninit();
          unsafe {
              let tmp_ptr: *mut Self = tmp.as_mut_ptr();
              let success = decode_image_data(binary_image_data.as_mut_ptr(), binary_image_data.len().try_into().unwrap(), tmp_ptr);
              if success {
                  Ok(tmp.assume_init())
              } else {
                  println!("Decode failed, returning 0");
                  let byte_ptr = tmp_ptr as *mut u8;
                  let size_in_bytes = std::mem::size_of::<Self>();
                  std::ptr::write_bytes(byte_ptr, 0, size_in_bytes);
                  Err(tmp.assume_init())
              }
          }
      }
  }

  impl Default for arg_input {
      fn default() -> Self {
          arg_input::new()
      }
  }
#+end_src

**** Implement methods for output class

***** List of output labels
#+begin_src rust :mkdirp yes :tangle ./src/model.rs
  const CLASS_LABELS: [&str; SIZE_O as usize] = ["empty", "occupied", "other"];
#+end_src

***** Actual functions
#+begin_src rust :mkdirp yes :tangle ./src/model.rs
  impl arg_output {

      pub fn new() -> Self {
          arg_output {
              val: [0.0; SIZE_O as usize],
          }
      }

      pub fn from<T: std::ops::Index<usize, Output = outtype>>(input: T) -> Self {
          let mut ret = arg_output::new();
          for i in 0..SIZE_O {
              ret.val[i as usize] = input[i as usize];
          }
          ret
      }

  }

  impl Default for arg_output {
      fn default() -> Self {
          arg_output::new()
      }
  }
#+end_src

**** Main inference function
#+begin_src rust :mkdirp yes :tangle ./src/model.rs
  async fn run_inference(mut input: Vec<arg_input>) -> Vec<arg_output> {
      let mut output: Vec<arg_output> = (0..input.len()).map(|_|{arg_output::new()}).collect(); 
      unsafe {
          mylibtorchinfer(input.as_mut_ptr(), input.len() as u32, output.as_mut_ptr());
      }
      output
  }
#+end_src

**** Reply class

***** Actual structure
#+begin_src rust :mkdirp yes :tangle ./src/model.rs
  #[derive(serde::Serialize)]
  pub struct prediction_probabilities_reply {
      val: [String; SIZE_O as usize],
      mj: String,
  }
#+end_src

***** Methods
#+begin_src rust :mkdirp yes :tangle ./src/model.rs
  impl prediction_probabilities_reply {
      pub fn new() -> Self {
          prediction_probabilities_reply {
              val: std::array::from_fn(|_| String::new()),
              mj: String::new(),
          }
      }

      pub fn from(input: arg_output) -> prediction_probabilities_reply {
          let mut max_index: usize = 0;
          let mut ret = prediction_probabilities_reply::new();

          ret.val[0] = input.val[0].to_string();
          for i in 1..SIZE_O {
              ret.val[i as usize] = input.val[i as usize].to_string();
              if input.val[i as usize] > input.val[max_index] {
                  max_index = i as usize;
              }
          }

          ret.mj = CLASS_LABELS[max_index].to_string();

          return ret;
      }
  }
#+end_src

**** Struct to send the inference request to the inferring thread
#+begin_src rust :mkdirp yes :tangle ./src/model.rs
  pub struct InferRequest {
      img: Box<arg_input>,
      resp_tx: tokio::sync::oneshot::Sender<Result<arg_output, String>>,
  }
#+end_src

**** Model server

***** Actual struct
#+begin_src rust :mkdirp yes :tangle ./src/model.rs
  pub struct model_server {
      rx: tokio::sync::mpsc::Receiver<InferRequest>,
  }
#+end_src

***** Important configs
#+begin_src rust :mkdirp yes :tangle ./src/model.rs
  const MAX_BATCH: usize = 16;
  const BATCH_TIMEOUT: std::time::Duration = std::time::Duration::from_millis(200);
#+end_src

***** Methods
#+begin_src rust :mkdirp yes :tangle ./src/model.rs
  impl model_server {
      pub async fn infer_loop(&mut self) {
          while let Some(first) = self.rx.recv().await {
              let mut images = Vec::with_capacity(MAX_BATCH);
              let mut reply_channel = Vec::with_capacity(MAX_BATCH);

              images.push(*(first.img));
              reply_channel.push(first.resp_tx);

              let start = tokio::time::Instant::now();
              while images.len() < MAX_BATCH && start.elapsed() < BATCH_TIMEOUT {
                  match self.rx.try_recv() {
                      Ok(req) => {
                          images.push(*(req.img));
                          reply_channel.push(req.resp_tx);
                      } ,
                      Err(_) => break,
                  }
              }

              let outputs = run_inference(images).await ;

              for (out, req) in outputs.into_iter().zip(reply_channel.into_iter()) {
                  let _ = req.send(Ok(out));
              }
          }
      }
  }
#+end_src

**** The model client

***** The Struct
#+begin_src rust :mkdirp yes :tangle ./src/model.rs
  pub struct model_client {
      tx: tokio::sync::mpsc::Sender<InferRequest>,
      preprocess: crate::mylib::image_processor,
  }
#+end_src

***** Implement the methods
#+begin_src rust :mkdirp yes :tangle ./src/model.rs
  impl model_client {
      pub async fn do_infer(
          &self,
          mut binary_image_data: Vec<u8>
      ) -> Result<arg_output, String> {

          let (resp_tx, resp_rx) = tokio::sync::oneshot::channel();

          let img = match arg_input::from(binary_image_data) {
              Ok(O) => {O} ,
              Err(E) => {println!("Failed to decode image, using blind 0s");E}
          } ;

          match self.tx.send(InferRequest{ img: img, resp_tx: resp_tx}).await {
              Ok(_) => match resp_rx.await {
                  Ok(Ok(pred)) => {
                      return Ok(pred);
                  }
                  Ok(Err(e)) => {
                      return Err(e);
                  }
                  Err(e) => {
                      return Err("Recv Error".to_string());
                  }
              },
              Err(e) => {
                  return Err("Send error".to_string());
              }
          }
      }

      pub async fn do_infer_data(&self, data: Vec<u8>) -> Result<arg_output, String> {
          return self.do_infer(data).await;
      }
  }
#+end_src

**** Convenient method to get both the server and client
#+begin_src rust :mkdirp yes :tangle ./src/model.rs
  pub fn get_inference_tuple() -> (model_server, model_client) {
      let (tx, rx) = tokio::sync::mpsc::channel::<InferRequest>(512);
      let ret_server = model_server {
          rx: rx,
      };
      let ret_client = model_client {
          tx: tx,
          preprocess: crate::mylib::image_processor::new(IMAGE_RESOLUTION),
      };
      return (ret_server, ret_client);
  }
#+end_src

*** Main code

**** Fixed
#+begin_src rust :mkdirp yes :tangle ./src/main.rs
  mod model;
  mod mylib;

  use futures_util::TryStreamExt;

  async fn infer_handler(
      mut payload: actix_multipart::Multipart,
      infer_slave: actix_web::web::Data<std::sync::Arc<model::model_client>>,
  ) -> Result<actix_web::HttpResponse, actix_web::Error> {

      let mut data = Vec::new();

      while let Some(mut field) = payload.try_next().await? {
          while let Some(chunk) = field.try_next().await? {
              data.extend_from_slice(&chunk);
          }
      }

      if data.is_empty() {
          return Ok(actix_web::HttpResponse::BadRequest().body("No image data"));
      }

      match infer_slave.do_infer_data(data).await {
          Ok(pred) => {
              return Ok(actix_web::HttpResponse::Ok().json(model::prediction_probabilities_reply::from(pred)));
          },
          Err(e) => {
              return Ok(actix_web::HttpResponse::InternalServerError().body(e));
          },
      }
  }

  pub mod infer {
      tonic::include_proto!("infer");
  }

  pub struct MyInferer {
      slave_client: std::sync::Arc<model::model_client>
  }

  #[tonic::async_trait]
  impl infer::infer_server::Infer for MyInferer {

      async fn do_infer(&self, request: tonic::Request<infer::Image>) -> Result<tonic::Response<infer::Prediction>, tonic::Status> {
          let image_data = request.into_inner().image_data;
          match self.slave_client.do_infer_data(image_data).await {
              Ok(pred) => {
                  let reply = infer::Prediction {
                      ps1: pred.val[0],
                      ps2: pred.val[1],
                      ps3: pred.val[2],
                  };
                  return Ok(tonic::Response::new(reply));
              },
              Err(e) => {
                  Err(tonic::Status::internal(e))
              },
          }
      }
  }
#+end_src

**** Actual functions to start actix and tonic

***** Actix
#+begin_src rust :mkdirp yes :tangle ./src/main.rs
  async fn main_actix(slave_client_1: std::sync::Arc<crate::model::model_client>) -> () {
      let port: u16 = 8000;
      match actix_web::HttpServer::new(
          move || {
            actix_web::App::new()
                .app_data(actix_web::web::Data::new(std::sync::Arc::clone(&slave_client_1)))
                .route("/infer", actix_web::web::post().to(infer_handler))
          }
      ).bind(("0.0.0.0", port)) {
          Ok(ret) => {
              println!("Actix binding to port {}", port);
              ret.run().await;
          }
          Err(e) => {
              eprintln!("Failed to bind to port");
          }
      }
  }
#+end_src

***** Tonic
#+begin_src rust :mkdirp yes :tangle ./src/main.rs
  async fn main_tonic(slave_client_2: std::sync::Arc<crate::model::model_client>) {
      let ip_v4 = std::net::IpAddr::V4(std::net::Ipv4Addr::new(0, 0, 0, 0));
      let port: u16 = 8001;
      let addr = std::net::SocketAddr::new(ip_v4, port);
      let inferer_service = MyInferer{slave_client: slave_client_2};
      println!("Starting tonic grpc server at port {}",port);
      tonic::transport::Server::builder().add_service(infer::infer_server::InferServer::new(inferer_service)).serve(addr).await;
  }
#+end_src

**** Simpler main
#+begin_src rust :mkdirp yes :tangle ./src/main.rs
  fn main() {
      let (mut slave_server, slave_client) = crate::model::get_inference_tuple();

      let rt = tokio::runtime::Builder::new_multi_thread()
          .thread_stack_size(1 << 24) 
          .enable_all()
          .build()
          .unwrap();

      rt.block_on(async {
          let slave_client_1 = std::sync::Arc::new(slave_client);
          let slave_client_2 = std::sync::Arc::clone(&slave_client_1);
          let future_infer = slave_server.infer_loop();
          let future_actix = main_actix(slave_client_1);
          let future_tonic = main_tonic(slave_client_2);
          tokio::join!(future_infer, future_actix, future_tonic);
      });
  }
#+end_src

* Script to run the rust code

** Add the file to git
#+begin_src sh :shebang #!/bin/sh :results output :mkdirp yes :tangle ./.git.sh
  GITADD 'run.sh'
  GITADD 'infer.sh'
#+end_src

** Run server
#+begin_src sh :shebang #!/bin/sh :results output :mkdirp yes :tangle ./run.sh
  cd "$(dirname -- "${0}")"
  export ROCR_VISIBLE_DEVICES=0
  './source/compile.sh'
  exec infer-server
#+end_src

** Run Client
#+begin_src sh :shebang #!/bin/sh :results output :mkdirp yes :tangle ./infer.sh
  cd '/data/source'
  infer-client
#+end_src

* Script to compile everything

** Add the file to git
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./.git.sh
  GITADD 'compile.sh'
#+end_src

** Actual script

*** Header
#+begin_src sh :shebang #!/bin/sh :results output :mkdirp yes :tangle ./compile.sh
  cd "$(dirname -- "${0}")"
  SRC="$(realpath .)"
  BLD="${SRC}/../build"
  mkdir -pv -- "${BLD}"
  BLD="$('realpath' "${BLD}")"
#+end_src

*** COMMENT Build c++
#+begin_src sh :shebang #!/bin/sh :results output :mkdirp yes :tangle ./compile.sh
  cd "${BLD}"
  cmake "${SRC}"
  make -j4
  make install
#+end_src

*** COMMENT Build rust
#+begin_src sh :shebang #!/bin/sh :results output :mkdirp yes :tangle ./compile.sh
  cd "${SRC}"
  export RUSTFLAGS="-C target-cpu=native"
  export CARGO_TARGET_DIR="${BLD}/cargo"
  mkdir -pv -- "${CARGO_TARGET_DIR}"
  bindgen './src/export.hpp' > './src/export.rs'
  cargo build --bin infer-server --release
  cargo build --bin infer-client --release
  cp -vf -- "${CARGO_TARGET_DIR}/release/infer-server" "${BLD}/"
  mkdir -pv -- '/usr/bin/'
  install --compare  "${CARGO_TARGET_DIR}/release/infer-server" '/usr/bin/infer-server'
  install --compare  "${CARGO_TARGET_DIR}/release/infer-client" '/usr/bin/infer-client'
#+end_src

*** Build pytorch model
#+begin_src sh :shebang #!/bin/sh :results output :mkdirp yes :tangle ./compile.sh
  cd "${SRC}"
  H="$(cat ./main.py | sha512sum | cut -d ' ' -f1)"
  mkdir -pv -- "${HOME}/.cache/${H}"
  test -e "${HOME}/.cache/${H}/model.pt2" || ./compile.py './model.ckpt' "${HOME}/.cache/${H}/model.pt2"
  ln -vfs -- "${HOME}/.cache/${H}/model.pt2" '/model.pt2'
#+end_src

*** Exit the script
#+begin_src sh :shebang #!/bin/sh :results output :mkdirp yes :tangle ./compile.sh
  exit '0'
#+end_src

* COMMENT Work space

** Just add
#+begin_src emacs-lisp :results silent
  (save-buffer)
  (org-babel-tangle)
  (async-shell-command "
      './.git.sh'
  " "log" "err")
#+end_src

** add commit and push
#+begin_src emacs-lisp :results silent
  (save-buffer)
  (org-babel-tangle)
  (async-shell-command "
      './.git.sh'
      git commit -m 'Fixed the bug of using host cpu options'
      ${HOME}/SSH/KEYS/PERSONAL_LAPTOP_PERSONAL_GITHUB/setup.sh
      git push
  " "log" "err")
#+end_src
