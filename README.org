* Scripts to manage
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./.git.sh
  . "${HOME}/important_functions.sh"

  CLEAN '#README.org#'
  CLEAN '.git.sh'
  CLEAN 'README.org~'

  GITADD 'README.org'
#+end_src

* Write a curl script for sample execution

** Add file to git
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./.git.sh
  GITADD 'curl.sh'
#+end_src

** Actual file
#+begin_src sh :shebang #!/bin/sh :results output :mkdirp yes :tangle ./curl.sh
  curl -X POST "http://127.0.0.1:8000/infer" -F "file=@./image.png"
  curl -X POST "http://127.0.0.1:8000/infer" -F "file=@./image.jpg"
#+end_src

* Script to build docker image

** Add file to git
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./.git.sh
  GITADD 'build_container.sh'
#+end_src

** Actual script to build the container
#+begin_src sh :shebang #!/bin/sh :results output :mkdirp yes :tangle ./build_container.sh
  cd "$('dirname' -- "${0}")"
  IMAGE_NAME='7_rust_libtorch_demo'
  mkdir -pv -- './build'

  H="$(cat './CMakeLists.txt' './Cargo.toml' './Dockerfile' './build.rs' './compile.py' './compile.sh' './curl.sh' './infer.proto' './infer.sh' './main.py' './run.sh' './src/client.rs' './src/export.hpp' './src/main.cpp' './src/main.hpp' './src/main.rs' './src/model.rs' './src/mylib.rs' './src/test.cpp' | sha512sum | cut -d ' ' -f1)"

  test -e "./build/${H}" && exit '0'
  CMD='sudo -A docker'
  which buildah && CMD='buildah'
  ${CMD} build -t "${IMAGE_NAME}" -f './Dockerfile' . && touch "./build/${H}"
  exit '0'
#+end_src

* Code to compile torch export artifacts

** Unify the files and add to git
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./.git.sh
  PREPARE_PYTHON_FILE 'compile_2_trt'
#+end_src

** Actual python code





*** config the import paths
#+begin_src python :shebang #!/usr/bin/env python3 :results output :mkdirp yes :tangle ./compile_2_trt.config.py
  import os

  try:
      __file__
  except:
      basepath = "."
  else:
      basepath = os.path.abspath(os.path.dirname(__file__) + "/")
  import sys

  sys.path.append(basepath)
#+end_src

*** Try importing torch_tensorrt
#+begin_src python :shebang #!/usr/bin/env python3 :results output :mkdirp yes :tangle ./compile_2_trt.config.py
  try:
      import torch_tensorrt

      HAVE_TRT = True
  except:
      HAVE_TRT = False
#+end_src

*** compile_2_trt.import.py
#+begin_src python :shebang #!/usr/bin/env python3 :results output :mkdirp yes :tangle ./compile_2_trt.import.py
  import torch
#+end_src

*** Function to compile

#+begin_src python :shebang #!/usr/bin/env python3 :results output :mkdirp yes
  compile_EP_to_tensorrt(
      path_file_input_EP_pt2="/root/source/model.pt2",
      path_file_output_trt_pt2="/root/model.pt2",
  )
#+end_src

**** To tensorrt
#+begin_src python :shebang #!/usr/bin/env python3 :results output :mkdirp yes :tangle ./compile_2_trt.function.py
  def compile_EP_to_tensorrt(
      path_file_input_EP_pt2,
      path_file_output_trt_pt2,
  ):
      device = "cuda"
      dtype = torch.bfloat16
      with torch.no_grad():
          EP = torch.export.load(path_file_input_EP_pt2)

          model = EP.module().to(
              device=device,
              dtype=dtype,
          )

          main_shape = tuple(EP.example_inputs[0][0].size())

          min_shape = tuple([1] + [main_shape[i] for i in range(1, len(main_shape))])
          opt_shape = tuple([8] + [main_shape[i] for i in range(1, len(main_shape))])
          max_shape = tuple([32] + [main_shape[i] for i in range(1, len(main_shape))])

          example_inputs = (
              torch.randn(
                  main_shape,
                  device=device,
                  dtype=dtype,
              ),
          )

          batch_dim = torch.export.Dim(
              "batch",
              min=1,
              max=32,
          )

          # [Optional] Specify the first dimension of the input x as dynamic.
          exported = torch.export.export(
              model,
              example_inputs,
              dynamic_shapes={"x": {0: batch_dim}},
          )
          # [Note] In this example we directly feed the exported module to aoti_compile_and_package.
          # Depending on your use case, e.g. if your training platform and inference platform
          # are different, you may choose to save the exported model using torch.export.save and
          # then load it back using torch.export.load on your inference platform to run AOT compilation.
          compile_settings = {
              "arg_inputs": [
                  torch_tensorrt.Input(
                      min_shape=min_shape,
                      opt_shape=opt_shape,
                      max_shape=max_shape,
                      dtype=dtype,
                  )
              ],
              "enabled_precisions": {dtype},
              "min_block_size": 1,
          }
          cg_trt_module = torch_tensorrt.dynamo.compile(exported, **compile_settings)
          torch_tensorrt.save(
              cg_trt_module,
              file_path=path_file_output_trt_pt2,
              output_format="aot_inductor",
              retrace=True,
              arg_inputs=example_inputs,
          )
#+end_src

**** To AOT Inductor
#+begin_src python :shebang #!/usr/bin/env python3 :results output :mkdirp yes :tangle ./compile_2_trt.function.py
  def compile_EP_to_AOTI(
      path_file_input_EP_pt2,
      path_file_output_AOTI_pt2,
  ):
      print("Inside the AOTI function")

      device = "cpu"
      dtype = torch.bfloat16
      inductor_configs = {}

      if torch.cuda.is_available():
          device = "cuda"
          inductor_configs["max_autotune"] = True

      ep = torch.export.load(path_file_input_EP_pt2)

      model = ep.module()

      model = model.to(
          device=device,
          dtype=dtype,
      )

      x = torch.randn(
          list(ep.example_inputs[0][0].size()),
          dtype=dtype,
          device=device,
      )

      dynamic_shapes = {
          "x": (
                  torch.export.dynamic_shapes.Dim.DYNAMIC,
                  torch.export.dynamic_shapes.Dim.STATIC,
                  torch.export.dynamic_shapes.Dim.STATIC,
                  torch.export.dynamic_shapes.Dim.STATIC,
          ),
      }

      exported_program = torch.export.export(
          model,
          (x,),
          dynamic_shapes=dynamic_shapes,
          strict=True,
      )

      path = torch._inductor.aoti_compile_and_package(
          exported_program,
          package_path=path_file_output_AOTI_pt2,
          inductor_configs=inductor_configs,
      )
#+end_src

**** Wrapper to run the correct compiling function
#+begin_src python :shebang #!/usr/bin/env python3 :results output :mkdirp yes :tangle ./compile_2_trt.function.py
  def compile_EP_2_optimized_targer(
      path_file_input_EP_pt2,
      path_file_output_compiled_pt2,
  ):
      if HAVE_TRT:
          compile_EP_to_tensorrt(
              path_file_input_EP_pt2=path_file_input_EP_pt2,
              path_file_output_trt_pt2=path_file_output_compiled_pt2,
          )
      else:
          compile_EP_to_AOTI(
              path_file_input_EP_pt2=path_file_input_EP_pt2,
              path_file_output_AOTI_pt2=path_file_output_compiled_pt2,
          )
#+end_src

*** Call the right compilation functions
#+begin_src python :shebang #!/usr/bin/env python3 :results output :mkdirp yes :tangle ./compile_2_trt.execute.py
  if __name__ == "__main__":
      compile_EP_2_optimized_targer(
          path_file_input_EP_pt2=sys.argv[1],
          path_file_output_compiled_pt2=sys.argv[2],
      )
#+end_src

* Python file to produce the model file

** Add the file to git
#+begin_src sh :shebang #!/bin/sh :results output :mkdirp yes :tangle ./.git.sh
  PREPARE_PYTHON_FILE 'main'
#+end_src

** Actual file

*** config the import paths
#+begin_src python :shebang #!/usr/bin/env python3 :results output :mkdirp yes :tangle ./main.config.py
  import os

  try:
      __file__
  except:
      basepath = "."
  else:
      basepath = os.path.abspath(os.path.dirname(__file__) + "/")
  import sys

  sys.path.append(basepath)
#+end_src

*** config standard variables
#+begin_src python :shebang #!/usr/bin/env python3 :results output :mkdirp yes :tangle ./main.config.py
  IMAGE_RESOLUTION = 448
  BATCH_SIZE = 4
  NUM_CHANNELS = 3
  NUM_CLASSES = 3

  (
      SIZE_B,
      SIZE_Y,
      SIZE_X,
      SIZE_C,
  ) = (
      BATCH_SIZE,
      IMAGE_RESOLUTION,
      IMAGE_RESOLUTION,
      NUM_CHANNELS,
  )

  INPUT_SHAPE = (
      SIZE_B,
      SIZE_Y,
      SIZE_X,
      SIZE_C,
  )
#+end_src

*** import important libraries
#+begin_src python :shebang #!/usr/bin/env python3 :results output :mkdirp yes :tangle ./main.import.py
  import os
  import sys
  import einops
  import timm
  import torch
  from torch.export.dynamic_shapes import Dim
#+end_src

*** class to define the actual model
#+begin_src python :shebang #!/usr/bin/env python3 :results output :mkdirp yes :tangle ./main.class.py
  class model_wrapper(torch.nn.Module):
      ################################################################
      ## Forward related functions BEGIN #############################
      ################################################################

      def forward_1_rearrange(
          self,
          x: torch.Tensor,
      ):
          x = einops.rearrange(
              x,
              "B Y X C -> B C Y X",
          )
          return x

      def forward_2_normalize(
          self,
          x: torch.Tensor,
      ):
          for i in range(SIZE_C):
              x[:, i, :, :] = ((x[:, i, :, :] / 255.0) - self.mean[i]) / self.std[i]

          return x

      def forward_3_backbone(
          self,
          x: torch.Tensor,
      ):
          x = self.timm_model(x)
          return x

      def forward_4_postprocess(
          self,
          x: torch.Tensor,
      ):
          x = torch.nn.functional.softmax(
              x,
              dim=1,
          )
          return x

      def forward(
          self,
          x: torch.Tensor,
      ):
          x = self.forward_1_rearrange(x)
          x = self.forward_2_normalize(x)
          x = self.forward_3_backbone(x)
          x = self.forward_4_postprocess(x)
          return x

      ################################################################
      ## Forward related functions END ###############################
      ################################################################

      ################################################################
      ## Init related functions BEGIN ################################
      ################################################################

      def init_setup_stat_parameters_as_float(self):
          self.mean = (
              0.48145466,
              0.4578275,
              0.40821073,
          )
          self.std = (
              0.26862954,
              0.26130258,
              0.27577711,
          )

      def init_timm_model(self):
          self.timm_model = timm.create_model(
              "timm/eva02_base_patch14_448.mim_in22k_ft_in1k",
              num_classes=NUM_CLASSES,
              pretrained=True,
          )

      def __init__(self):
          super().__init__()
          self.init_setup_stat_parameters_as_float()
          self.init_timm_model()

      ################################################################
      ## Init related functions END ##################################
      ################################################################
#+end_src

*** function to produce the export
#+begin_src python :shebang #!/usr/bin/env python3 :results output :mkdirp yes :tangle ./main.function.py
  def produce_model_ep(path_file_output_model_ep):
      dtype = torch.bfloat16
      model = model_wrapper()
      model.eval()
      with torch.inference_mode():
          if torch.cuda.is_available():
              device = "cuda"
          else:
              device = "cpu"

          print("device = ", device)
          print("dtype = ", dtype)

          model = model.to(device=device, dtype=dtype)
          model = torch.compile(model)
          x = torch.rand(
              INPUT_SHAPE,
              dtype=dtype,
              device=device,
          )

          dynamic_shapes = {
              "x": (
                  Dim.DYNAMIC,
                  Dim.STATIC,
                  Dim.STATIC,
                  Dim.STATIC,
              ),
          }

          exported_program = torch.export.export(
              model._orig_mod,
              # model,
              (x,),
              dynamic_shapes=dynamic_shapes,
              strict=True,
          )
          torch.export.save(
              ep=exported_program,
              f=path_file_output_model_ep,
          )

          # path = torch._inductor.aoti_compile_and_package(
          #     exported_program,
          #     package_path=path_file_out,
          #     inductor_configs=inductor_configs,
          # )
#+end_src

*** COMMENT function to test if it works
#+begin_src python :shebang #!/usr/bin/env python3 :results output :mkdirp yes :tangle ./main.function.py
  def test_model(path_file_input_model):
      dtype = torch.bfloat16
      if torch.cuda.is_available():
          device = "cuda"
      else:
          device = "cpu"
      model = torch._inductor.aoti_load_package(path_file_in)
      x = torch.rand(
          (
              SIZE_B * 2,
              SIZE_Y,
              SIZE_X,
              SIZE_C,
          ),
          dtype=dtype,
          device=device,
      )
      with torch.inference_mode():
          output = model(x)

          print(output)
      return output
#+end_src

*** COMMENT Function wrapper to maintain compatibility
#+begin_src python :shebang #!/usr/bin/env python3 :results output :mkdirp yes :tangle ./main.function.py
  def compile_model_to_aot_inductor(path_file_input, path_file_output):
      produce_model(path_file_out=path_file_output)
#+end_src

*** execute
#+begin_src python :shebang #!/usr/bin/env python3 :results output :mkdirp yes :tangle ./main.execute.py
  if __name__ == '__main__':
      produce_model_ep(path_file_output_model_ep=sys.argv[1])
#+end_src

* Python program to export the model

** Add file to git
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./.git.sh
  GITADD 'compile.py'
#+end_src

** Actual file
#+begin_src python :shebang #!/usr/bin/env python3 :results output :mkdirp yes :tangle ./compile.py
  from main import produce_model_ep
  from compile_2_trt import compile_EP_2_optimized_targer
  import sys

  if __name__ == "__main__":
      produce_model_ep(path_file_output_model_ep=sys.argv[1])
      compile_EP_2_optimized_targer(
          path_file_input_EP_pt2=sys.argv[1],
          path_file_output_compiled_pt2=sys.argv[2],
      )
#+end_src

* Dockerfile to produce single self contained image

** Add docker file to git
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./.git.sh
  GITADD 'Dockerfile'
#+end_src

** Actual docker file
#+begin_src conf :mkdirp yes :tangle ./Dockerfile
  FROM 6_pytorch

  RUN mkdir -pv -- '/root/source' '/root/build'

  COPY './CMakeLists.txt' '/root/source/CMakeLists.txt'
  COPY './Cargo.toml' '/root/source/Cargo.toml'
  COPY './build.rs' '/root/source/build.rs'
  COPY './compile.py' '/root/source/compile.py'
  COPY './compile.sh' '/root/source/compile.sh'
  COPY './compile_2_trt.py' '/root/source/compile_2_trt.py'
  COPY './curl.sh' '/root/curl.sh'
  COPY './infer.proto' '/root/source/infer.proto'
  COPY './infer.sh' '/root/infer.sh'
  COPY './main.py' '/root/source/main.py'
  COPY './run.sh' '/root/run.sh'
  COPY './src/client.rs' '/root/source/src/client.rs'
  COPY './src/export.hpp' '/root/source/src/export.hpp'
  COPY './src/main.cpp' '/root/source/src/main.cpp'
  COPY './src/main.hpp' '/root/source/src/main.hpp'
  COPY './src/main.rs' '/root/source/src/main.rs'
  COPY './src/model.rs' '/root/source/src/model.rs'
  COPY './src/mylib.rs' '/root/source/src/mylib.rs'
  COPY './src/test.cpp' '/root/source/src/test.cpp'

  RUN \
      echo 'START Compile c++ parts' \
      && cd '/root/build' \
      && cmake '../source' \
      && make -j4 \
      && make install \
      && echo 'DONE Compile c++ parts' ;


  ENV RUSTFLAGS="-C target-cpu=native"
  ENV CARGO_TARGET_DIR="/root/build/cargo"

  RUN \
      echo 'START Compiling rust parts' \
      && cd '/root/source' \
      && bindgen './src/export.hpp' > './src/export.rs' \
      && cargo build --bin infer-server --release \
      && cargo build --bin infer-client --release \
      && install --compare  "${CARGO_TARGET_DIR}/release/infer-server" '/usr/bin/infer-server' \
      && install --compare  "${CARGO_TARGET_DIR}/release/infer-client" '/usr/bin/infer-client' \
      && echo 'DONE Compiling rust parts' ;

  RUN '/root/source/main.py' '/root/source/model.pt2'
#+end_src

* Scripts to run image

** Add file to git
#+begin_src sh :shebang #!/bin/sh :results output :mkdirp yes :tangle ./.git.sh
  GITADD 'image_run.sh'
  GITADD 'image_run_amd.sh'
  GITADD 'image_run_nvidia.sh'
#+end_src

** Actual script

*** For cpu
#+begin_src sh :shebang #!/bin/sh :results output :mkdirp yes :tangle ./image_run.sh
  cd "$(dirname -- "${0}")"

  mkdir -pv -- "${HOME}/BUILD"

  IMAGE_NAME='7_rust_libtorch_demo'

  RUN_CONTAINER () {
      CMD='sudo -A docker'
      which podman && CMD='podman'
      ${CMD} run \
          -it --rm \
          '--security-opt' 'seccomp=unconfined' \
          --mount 'type=tmpfs,destination=/data/TMPFS,tmpfs-size=137438953472' \
          -v "$(realpath .):/data/source" \
          -v "${HOME}/BUILD:/data/build" \
          -v "CACHE:/usr/local/cargo/registry" \
          -v "CACHE:/root/.cache" \
          "${IMAGE_NAME}" zsh \
      ;
  }

  RUN_CONTAINER
#+end_src

*** For amd gpu
#+begin_src sh :shebang #!/bin/sh :results output :mkdirp yes :tangle ./image_run_amd.sh
  cd "$(dirname -- "${0}")"

  mkdir -pv -- "${HOME}/BUILD"

  IMAGE_NAME='7_rust_libtorch_demo'

  RUN_CONTAINER () {
      CMD='sudo -A docker'
      which podman && CMD='podman'
      ${CMD} run \
          -it --rm \
          '--device' '/dev/kfd' \
          '--device' '/dev/dri' \
          '--security-opt' 'seccomp=unconfined' \
          --mount 'type=tmpfs,destination=/data/TMPFS,tmpfs-size=137438953472' \
          -v "$(realpath .):/data/source" \
          -v "${HOME}/BUILD:/data/build" \
          -v "CACHE:/usr/local/cargo/registry" \
          -v "CACHE:/root/.cache" \
          -e 'ROCR_VISIBLE_DEVICES=0' \
          "${IMAGE_NAME}" zsh \
      ;
  }

  RUN_CONTAINER
#+end_src

*** For nvidia gpu
#+begin_src sh :shebang #!/bin/sh :results output :mkdirp yes :tangle ./image_run_nvidia.sh
  cd "$(dirname -- "${0}")"

  mkdir -pv -- "${HOME}/BUILD"

  IMAGE_NAME='7_rust_libtorch_demo'

  RUN_CONTAINER () {
      CMD='sudo -A docker'
      which podman && CMD='podman'
      ${CMD} run \
          --tty \
          --interactive \
          --rm \
          --gpus 'all,"capabilities=compute,utility,video"' \
          --ipc host \
          --ulimit memlock=-1 \
          --ulimit stack=67108864 \
          --shm-size 107374182400 \
          --mount 'type=tmpfs,destination=/data/TMPFS,tmpfs-size=137438953472' \
          -v "$(realpath .):/data/source" \
          -v "${HOME}/BUILD:/data/build" \
          -v "CACHE:/usr/local/cargo/registry" \
          -v "CACHE:/root/.cache" \
          -v "CACHE:/root/.triton" \
          "${IMAGE_NAME}" zsh \
      ;
  }

  RUN_CONTAINER
#+end_src

* Main c++ - rust interface

** Add the file to git
#+begin_src sh :shebang #!/bin/sh :results output :mkdirp yes :tangle ./.git.sh
  GITADD 'src/export.hpp'
  GITADD 'src/export.rs'
#+end_src

** Actual interface headers

*** C++ header
#+begin_src c++ :mkdirp yes :tangle ./src/export.hpp
  #ifndef _HEADER_GUARD_src_export_hpp_
  #define _HEADER_GUARD_src_export_hpp_

  extern "C" {

  unsigned int constexpr IMAGE_RESOLUTION = 448;
  unsigned int constexpr NUM_CHANNELS = 3;
  unsigned int constexpr NUM_CLASSES = 3;

  unsigned int constexpr SIZE_Y = IMAGE_RESOLUTION;
  unsigned int constexpr SIZE_X = IMAGE_RESOLUTION;
  unsigned int constexpr SIZE_C = NUM_CHANNELS;
  unsigned int constexpr SIZE_O = NUM_CLASSES;

  typedef unsigned char intype;
  typedef float outtype;

  struct arg_input {
    intype val[SIZE_Y][SIZE_X][SIZE_C];
  };

  struct arg_output {
    outtype val[SIZE_O];
  };

  void mylibtorchinfer(arg_input *in, unsigned int const batch_size,
                       arg_output *out);

  bool decode_image_data(unsigned char *binary_data, int data_size,
                         arg_input *dst_struct);
  }

  #endif
#+end_src

*** rs header
#+begin_src c++ :mkdirp yes :tangle ./src/export.rs
  pub const IMAGE_RESOLUTION: ::std::os::raw::c_uint = 448;
  pub const NUM_CHANNELS: ::std::os::raw::c_uint = 3;
  pub const NUM_CLASSES: ::std::os::raw::c_uint = 3;
  pub const SIZE_Y: ::std::os::raw::c_uint = 448;
  pub const SIZE_X: ::std::os::raw::c_uint = 448;
  pub const SIZE_C: ::std::os::raw::c_uint = 3;
  pub const SIZE_O: ::std::os::raw::c_uint = 3;
  pub type intype = ::std::os::raw::c_uchar;
  pub type outtype = f32;
  #[repr(C)]
  #[derive(Debug, Copy, Clone)]
  pub struct arg_input {
      pub val: [[[intype; 3usize]; 448usize]; 448usize],
  }
  #[allow(clippy::unnecessary_operation, clippy::identity_op)]
  const _: () = {
      ["Size of arg_input"][::std::mem::size_of::<arg_input>() - 602112usize];
      ["Alignment of arg_input"][::std::mem::align_of::<arg_input>() - 1usize];
      ["Offset of field: arg_input::val"][::std::mem::offset_of!(arg_input, val) - 0usize];
  };
  #[repr(C)]
  #[derive(Debug, Copy, Clone)]
  pub struct arg_output {
      pub val: [outtype; 3usize],
  }
  #[allow(clippy::unnecessary_operation, clippy::identity_op)]
  const _: () = {
      ["Size of arg_output"][::std::mem::size_of::<arg_output>() - 12usize];
      ["Alignment of arg_output"][::std::mem::align_of::<arg_output>() - 4usize];
      ["Offset of field: arg_output::val"][::std::mem::offset_of!(arg_output, val) - 0usize];
  };
  unsafe extern "C" {
      pub fn mylibtorchinfer(
          in_: *mut arg_input,
          batch_size: ::std::os::raw::c_uint,
          out: *mut arg_output,
      );
  }
  unsafe extern "C" {
      pub fn decode_image_data(
          binary_data: *mut ::std::os::raw::c_uchar,
          data_size: ::std::os::raw::c_int,
          dst_struct: *mut arg_input,
      ) -> bool;
  }
#+end_src

* Main c++ implementation part

** Include all the main headers

*** Add the file to git
#+begin_src sh :shebang #!/bin/sh :results output :mkdirp yes :tangle ./.git.sh
  GITADD 'src/main.hpp'
#+end_src

*** Actual file
#+begin_src c++ :mkdirp yes :tangle ./src/main.hpp
  #ifndef _HEADER_GUARD_src_main_hpp
  #define _HEADER_GUARD_src_main_hpp

  #include "./export.hpp"
  #include <iostream>
  #include <opencv2/core/mat.hpp>
  #include <opencv2/imgcodecs.hpp>
  #include <opencv2/opencv.hpp>
  #include <torch/csrc/inductor/aoti_package/model_package_loader.h>
  #include <torch/torch.h>
  #include <vector>

  #endif
#+end_src

** Main C++ code

*** Add the file to git
#+begin_src sh :shebang #!/bin/sh :results output :mkdirp yes :tangle ./.git.sh
  GITADD 'src/main.cpp'
#+end_src

*** Actual file

**** Header guard start
#+begin_src c++ :mkdirp yes :tangle ./src/main.cpp
  #ifndef _HEADER_GUARD_src_main_cpp
  #define _HEADER_GUARD_src_main_cpp
#+end_src

**** Include header
#+begin_src c++ :mkdirp yes :tangle ./src/main.cpp
  #include "./main.hpp"
#+end_src

**** CV parts

***** Decode binary data to cv::Mat with resizing, cropping and RGB_2_BGR
#+begin_src c++ :mkdirp yes :tangle ./src/main.cpp
  inline cv::Mat process_image_data(unsigned char *binary_data, int data_size) {
    cv::Mat decoded_img = cv::imdecode(
        cv::Mat(1, data_size, CV_8UC1, binary_data), cv::IMREAD_COLOR);

    if (decoded_img.empty()) {
      return cv::Mat::zeros(SIZE_Y, SIZE_X, CV_8UC1);
    }

    int const height = decoded_img.rows;
    int const width = decoded_img.cols;
    int target_height = IMAGE_RESOLUTION;
    int target_width = IMAGE_RESOLUTION;
    int x_start = 0;
    int y_start = 0;
    float ar = 1;

    if (height < width) {
      ar = float(width) / float(height);
      target_width = int(float(IMAGE_RESOLUTION) * ar);
      x_start = static_cast<unsigned int>(target_width - IMAGE_RESOLUTION) >> 1;
    } else {
      ar = float(height) / float(width);
      target_height = int(float(IMAGE_RESOLUTION) * ar);
      y_start = static_cast<unsigned int>(target_height - IMAGE_RESOLUTION) >> 1;
    }

    cv::Mat resized_img;
    if ((height > IMAGE_RESOLUTION) && (width > IMAGE_RESOLUTION)) {
      cv::resize(decoded_img, resized_img, cv::Size(target_width, target_height),
                 0, 0, cv::INTER_AREA);

    } else {
      cv::resize(decoded_img, resized_img, cv::Size(target_width, target_height),
                 0, 0, cv::INTER_LANCZOS4);

    }

    cv::Rect roi(x_start, y_start, IMAGE_RESOLUTION, IMAGE_RESOLUTION);
    cv::Mat cropped_img = resized_img(roi);

    if (false) {
      return cropped_img;
    } else {
      cv::Mat rgb_img;
      cv::cvtColor(cropped_img, rgb_img, cv::COLOR_BGR2RGB);
      return rgb_img;
    }
  }
#+end_src

***** Convert the cv::Mar to arg_input
#+begin_src c++ :mkdirp yes :tangle ./src/main.cpp
  inline bool convertMatToStruct(const cv::Mat &src_mat, arg_input &dst_struct) {
    if (src_mat.rows != SIZE_Y || src_mat.cols != SIZE_X) {
      return false;
    }

    if (src_mat.type() != CV_8UC3) {
      return false;
    }

    if (src_mat.isContinuous()) {
      constexpr size_t EXPECTED_SIZE_BYTES =
          SIZE_Y * SIZE_X * SIZE_C * sizeof(intype);

      const uint8_t *mat_data_ptr = src_mat.data;

      uint8_t *struct_data_ptr = reinterpret_cast<uint8_t *>(dst_struct.val);

      std::memcpy(struct_data_ptr, mat_data_ptr, EXPECTED_SIZE_BYTES);

    } else {

      constexpr size_t ROW_SIZE_BYTES = SIZE_X * SIZE_C * sizeof(intype);

      if (false) {
        for (int y = 0; y < SIZE_Y; ++y) {
          const uint8_t *src_row = src_mat.ptr<uint8_t>(y);
          for (int x = 0; x < SIZE_X; ++x) {
            for (int c = 0; c < SIZE_C; ++c) {
              dst_struct.val[y][x][c] = src_row[(x * SIZE_C) + (SIZE_C - 1 - c)];
            }
          }
        }
      } else {
        for (int y = 0; y < SIZE_Y; ++y) {
          const uint8_t *src_row = src_mat.ptr<uint8_t>(y);
          uint8_t *dst_row = reinterpret_cast<uint8_t *>(dst_struct.val[y]);
          std::memcpy(dst_row, src_row, ROW_SIZE_BYTES);
        }
      }
    }

    return true;
  }
#+end_src

**** Torch parts

***** Dtype parts
#+begin_src c++ :mkdirp yes :tangle ./src/main.cpp
  using float32_t = float;
  using float64_t = double;

  template <typename T> inline auto get_tensor_dtype() {
    return torch::kBFloat16;
  }

  template <> inline auto get_tensor_dtype<uint8_t>() {
    return torch::kUInt8;
  }

  template <> inline auto get_tensor_dtype<uint16_t>() {
    return torch::kUInt16;
  }

  template <> inline auto get_tensor_dtype<uint32_t>() {
    return torch::kUInt32;
  }

  template <> inline auto get_tensor_dtype<uint64_t>() {
    return torch::kInt64;
  }

  template <> inline auto get_tensor_dtype<int8_t>() {
    return torch::kInt8;
  }

  template <> inline auto get_tensor_dtype<int16_t>() {
    return torch::kInt16;
  }

  template <> inline auto get_tensor_dtype<int32_t>() {
    return torch::kInt32;
  }

  template <> inline auto get_tensor_dtype<int64_t>() {
    return torch::kInt64;
  }

  template <> inline auto get_tensor_dtype<float32_t>() {
    return torch::kFloat32;
  }

  template <> inline auto get_tensor_dtype<float64_t>() {
    return torch::kFloat64;
  }
#+end_src

***** Torch stuff
#+begin_src c++ :mkdirp yes :tangle ./src/main.cpp
  inline std::string get_model_path() {
    printf("called get_model_path()\n");
    return std::string("/model.pt2");
  }

  inline torch::TensorOptions get_good_device_and_dtype() {
    printf("Called get_good_device_and_dtype()\n");
    if (torch::cuda::is_available()) {
      printf("Returning cuda\n");
      return torch::TensorOptions().dtype(torch::kBFloat16).device(torch::kCUDA);
    } else {
      printf("Returning cpu\n");
      return torch::TensorOptions().dtype(torch::kBFloat16).device(torch::kCPU);
    }
  }

  inline torch::TensorOptions get_host_input_device_and_dtype() {
    printf("Called get_host_input_device_and_dtype()\n");
    return torch::TensorOptions()
        .dtype(get_tensor_dtype<intype>())
        .device(torch::kCPU);
  }

  inline torch::TensorOptions get_host_output_device_and_dtype() {
    printf("get_host_output_device_and_dtype started\n");
    return torch::TensorOptions()
        .dtype(get_tensor_dtype<outtype>())
        .device(torch::kCPU);
  }

  class infer_slave {
  private:
    c10::InferenceMode mode;
    torch::inductor::AOTIModelPackageLoader loader;
    torch::TensorOptions options_compute;
    torch::TensorOptions options_host_input;
    torch::TensorOptions options_host_output;
    torch::Tensor input_tensor;
    std::vector<torch::Tensor> inputs;
    std::vector<torch::Tensor> outputs;
    torch::Tensor out_tensor;
    std::size_t bytes_to_copy;

  public:
    inline void operator()(arg_input *in, unsigned int const batch_size,
                           arg_output *out) {
      printf("Inside the inference function\n");
      torch::Tensor cpu_tensor = torch::from_blob(
          static_cast<void *>(in), {batch_size, SIZE_Y, SIZE_X, SIZE_C},
          options_host_input);
      printf("Step-1\n");
      inputs[0] = cpu_tensor.to(options_compute);
      printf("Step-2\n");
      outputs = loader.run(inputs);
      printf("Step-3\n");
      out_tensor = outputs[0].contiguous().cpu().to(options_host_output);
      printf("Step-4\n");
      bytes_to_copy = batch_size * SIZE_O * sizeof(outtype);
      printf("Step-5\n");
      std::memcpy(out, out_tensor.data_ptr<outtype>(), bytes_to_copy);
      printf("Step-6\n");
    }

    infer_slave()
        : loader(get_model_path()), options_compute(get_good_device_and_dtype()),
          options_host_input(get_host_input_device_and_dtype()),
          options_host_output(get_host_output_device_and_dtype()) {
      printf("Started actual constructor\n");
      inputs.resize(1);
      printf("Done constructing...\n");
    }

    ~infer_slave() {}
  };

  infer_slave slave;
#+end_src

**** Actual interface implementation
#+begin_src c++ :mkdirp yes :tangle ./src/main.cpp
  extern "C" {

  void mylibtorchinfer(arg_input *in, unsigned int const batch_size,
                       arg_output *out) {

    slave(in, batch_size, out);
  }

  bool decode_image_data(unsigned char *binary_data, int data_size,
                         arg_input *dst_struct) {

    /*inline*/ cv::Mat ret =
        process_image_data(/*unsigned char *binary_data =*/binary_data,
                           /*int data_size =*/data_size);

    /*inline*/ bool res = convertMatToStruct(
        /*const cv::Mat& src_mat =*/ret, /*arg_input& dst_struct =*/*dst_struct);

    return res;
  }
  }
#+end_src

**** Header guard end
#+begin_src c++ :mkdirp yes :tangle ./src/main.cpp
  #endif
#+end_src

** Include main function to test

*** Add the file to git
#+begin_src sh :shebang #!/bin/sh :results output :mkdirp yes :tangle ./.git.sh
  GITADD './src/test.cpp'
#+end_src

*** Call the function
#+begin_src c++ :mkdirp yes :tangle ./src/test.cpp
  #include "./main.cpp"
  int main(){
    printf("Inside main\n");
    arg_input in;
    arg_output out;

    printf("Started allocating\n");
    for(int y = 0 ; y < IMAGE_RESOLUTION; ++y){
      for(int x=0; x<IMAGE_RESOLUTION; ++x){
        for(int c=0; c<NUM_CHANNELS; ++c){
          in.val[y][x][c] = 0;
        }
      }
    }

    printf("Allocated the arg_input\n");

    mylibtorchinfer(/*arg_input *in =*/ &in, /*unsigned int const batch_size =*/ 1, /*arg_output *out =*/ &out);
    printf("Returned from the inference function\n");
    for(int i = 0; i< NUM_CLASSES; ++i){printf("%lf, ",out.val[i]);}
    printf("\n");
    
    return 0;
  }
#+end_src

** CMake parts

*** Add the file to git
#+begin_src sh :shebang #!/bin/sh :results output :mkdirp yes :tangle ./.git.sh
  GITADD 'CMakeLists.txt'
  GITADD 'CMakeLists_trt.txt'
#+end_src

*** Actual file

fd 'torchtrt_runtime\.so|nvinfer\.so|nvinfer_plugin\.so' ~/venv -x cp {} /lib/

**** For torch TRT
#+begin_src conf :mkdirp yes :tangle ./CMakeLists_trt.txt
  cmake_minimum_required(VERSION 3.18 FATAL_ERROR)

  project(aoti_example VERSION 1.0 DESCRIPTION "Interface for inferring with libtorch exposed to rust")

  find_package(Torch REQUIRED)
  find_package(OpenCV REQUIRED)

  add_executable(aoti_example src/test.cpp)
  add_library(mytorch SHARED src/main.cpp)

  target_link_libraries(aoti_example "${TORCH_LIBRARIES}" "${OpenCV_LIBS}" torchtrt_runtime nvinfer nvinfer_plugin)
  # target_link_libraries(torchtrt_aoti_example "${TORCH_LIBRARIES}" "-Wl,--no-as-needed" torchtrt_runtime "-Wl,--as-needed")

  target_link_libraries(mytorch "${TORCH_LIBRARIES}" "${OpenCV_LIBS}")

  set_property(TARGET aoti_example PROPERTY CXX_STANDARD 17)
  set_target_properties(mytorch PROPERTIES VERSION ${PROJECT_VERSION} CXX_STANDARD 17)

  install(TARGETS mytorch LIBRARY DESTINATION lib)

  set(CMAKE_C_FLAGS "${CMAKE_C_FLAGS} -O3 -mtune=native -march=native -flto")
  set(CMAKE_CXX_FLAGS "${CMAKE_C_FLAGS} -O3 -mtune=native -march=native -flto")
  set(CMAKE_INSTALL_PREFIX "/usr")
  set(CMAKE_EXPORT_COMPILE_COMMANDS 1)
#+end_src

**** The generic cmake file
#+begin_src conf :mkdirp yes :tangle ./CMakeLists.txt
  cmake_minimum_required(VERSION 3.18 FATAL_ERROR)

  project(aoti_example VERSION 1.0 DESCRIPTION "Interface for inferring with libtorch exposed to rust")

  find_package(Torch REQUIRED)
  find_package(OpenCV REQUIRED)

  add_executable(aoti_example src/test.cpp)
  add_library(mytorch SHARED src/main.cpp)

  target_link_libraries(aoti_example "${TORCH_LIBRARIES}" "${OpenCV_LIBS}")
  target_link_libraries(mytorch "${TORCH_LIBRARIES}" "${OpenCV_LIBS}")

  set_property(TARGET aoti_example PROPERTY CXX_STANDARD 17)
  set_target_properties(mytorch PROPERTIES VERSION ${PROJECT_VERSION} CXX_STANDARD 17)

  install(TARGETS mytorch LIBRARY DESTINATION lib)

  set(CMAKE_C_FLAGS "${CMAKE_C_FLAGS} -O3 -mtune=native -march=native -flto")
  set(CMAKE_CXX_FLAGS "${CMAKE_C_FLAGS} -O3 -mtune=native -march=native -flto")
  set(CMAKE_INSTALL_PREFIX "/usr")
  set(CMAKE_EXPORT_COMPILE_COMMANDS 1)
#+end_src

* Cargo.toml

** Add the file to git
#+begin_src sh :shebang #!/bin/sh :results output :mkdirp yes :tangle ./.git.sh
  GITADD 'Cargo.toml'
#+end_src

** Actual file

*** Main metadata
#+begin_src toml :mkdirp yes :tangle ./Cargo.toml
  [package]
  name = "burn_test"
  version = "0.1.0"
  edition = "2024"

  include = [
      "src/main.cpp",
      "src/main.hpp",
      "src/main.rs",
      "Cargo.toml",
      "build.rs",
      "compile.py",
  ]
#+end_src

*** List of files to compile

**** Server
#+begin_src toml :mkdirp yes :tangle ./Cargo.toml
  [[bin]]
  name = "infer-server"
  path = "src/main.rs"
#+end_src

**** GRPC Client
#+begin_src toml :mkdirp yes :tangle ./Cargo.toml
  [[bin]]
  name = "infer-client"
  path = "src/client.rs"
#+end_src

*** List of build dependencies
#+begin_src toml :mkdirp yes :tangle ./Cargo.toml
  [build-dependencies]
  tonic-prost-build = "0.14.2"
#+end_src

*** List of dependencies
#+begin_src toml :mkdirp yes :tangle ./Cargo.toml
  [dependencies]
#+end_src

**** Actix
#+begin_src toml :mkdirp yes :tangle ./Cargo.toml
  actix-multipart = "0.7.2"
  actix-web = "4.11.0"
#+end_src

**** GRPC Related
#+begin_src toml :mkdirp yes :tangle ./Cargo.toml
  prost = "0.14"
  tonic-prost = "*"
  tonic = { version = "0.14.2", features = ["zstd"] }
#+end_src

**** Async
#+begin_src toml :mkdirp yes :tangle ./Cargo.toml
  futures-util = "0.3.31"
  tokio = { version = "1.47.1", features = ["full"] }
#+end_src

**** Serialization
#+begin_src toml :mkdirp yes :tangle ./Cargo.toml
  serde = { version = "1.0.219", features = ["derive"] }
#+end_src

**** Image processing
#+begin_src toml :mkdirp yes :tangle ./Cargo.toml
  image = { version = "0.25.6", features = ["serde", "nasm"] }
#+end_src

**** Memory allocator

***** COMMENT rpmalloc
#+begin_src toml :mkdirp yes :tangle ./Cargo.toml
  rpmalloc = { version = "0.2.2", features = ["adaptive_thread_cache", "global_cache", "thread_cache"] }
#+end_src

***** COMMENT jemalloc
#+begin_src toml :mkdirp yes :tangle ./Cargo.toml
  tikv-jemallocator = "0.6.1"
#+end_src

***** mimalloc
#+begin_src toml :mkdirp yes :tangle ./Cargo.toml
  mimalloc = "0.1.48"
#+end_src

* proto file

** Add the file
#+begin_src sh :shebang #!/bin/sh :results output :mkdirp yes :tangle ./.git.sh
  GITADD 'infer.proto'
#+end_src

** Actual file
#+begin_src rust :tangle ./infer.proto
  syntax = "proto3";

  package infer;

  message Image {
      bytes image_data = 1;
  }

  message Prediction {
      float ps1 = 1;
      float ps2 = 2;
      float ps3 = 3;
  }

  service Infer {
    rpc doInfer(Image) returns (Prediction) {}
  }
#+end_src

* build.rs

** Add the file to git
#+begin_src sh :shebang #!/bin/sh :results output :mkdirp yes :tangle ./.git.sh
  GITADD 'build.rs'
#+end_src

** Actual file
#+begin_src rust :mkdirp yes :tangle ./build.rs
  fn main() -> Result<(), Box<dyn std::error::Error>> {
      tonic_prost_build::compile_protos("./infer.proto")?;
      println!("cargo:rustc-link-arg=-lmytorch");
      Ok(())
  }
#+end_src

* GRPC inference code

** Add files to git
#+begin_src sh :shebang #!/bin/sh :results output :mkdirp yes :tangle ./.git.sh
  GITADD 'src/client.rs'
#+end_src

** Code for client
#+begin_src rust :mkdirp yes :tangle ./src/client.rs
  pub mod infer {
      tonic::include_proto!("infer"); // The string specified here must match the proto package name
  }

  use std::fs;

  #[tokio::main]
  async fn main() -> Result<(), Box<dyn std::error::Error>> {
      let data = fs::read("./image.jpg").expect("Failed reading image file");
      let img = infer::Image { image_data: data };
      let mut client = infer::infer_client::InferClient::connect("http://127.0.0.1:8001").await?;
      let res = client.do_infer(img).await?;
      println!("{:?}", res);
      return Ok(());
  }
#+end_src

* Main rust code

** Add files to git
#+begin_src sh :shebang #!/bin/sh :results output :mkdirp yes :tangle ./.git.sh
  GITADD 'src/main.rs'
  GITADD 'src/model.rs'
  GITADD 'src/mylib.rs'
#+end_src

** Actual code

*** lib file
#+begin_src rust :mkdirp yes :tangle ./src/mylib.rs
  pub struct image_processor {
      image_resolution: u32,
  }

  impl image_processor {

      pub fn new(val: u32) -> Self {
          return image_processor {
              image_resolution: val,
          };
      }

      fn preprocess(&self, img:  image::DynamicImage) -> image::RgbaImage {
          let (width, height) = (img.width(), img.height());
          let size = width.min(height);
          let x = (width - size) / 2;
          let y = (height - size) / 2;
          let cropped_img = image::imageops::crop_imm(&img, x, y, size, size).to_image();
          image::imageops::resize(
              &cropped_img,
              self.image_resolution,
              self.image_resolution,
              image::imageops::FilterType::CatmullRom,
          )
      }

      pub fn decode_and_preprocess(&self, data: Vec<u8>) -> Result<image::RgbaImage, String> {
          match image::load_from_memory(&data) {
              Ok(img) => {
                  return Ok(self.preprocess(img));
              }
              Err(e) => {
                  return Err("decode error".to_string());
              }
          };
      }
  }
#+end_src

*** Model code

**** Outputs from bindgen
#+begin_src rust :mkdirp yes :tangle ./src/model.rs
  include!("export.rs");
#+end_src

**** Implement methods for input class
#+begin_src rust :mkdirp yes :tangle ./src/model.rs
  impl arg_input {
      pub fn new() -> Self {
          arg_input {
              val: [[[0 as intype; SIZE_C as  usize]; SIZE_X as usize]; SIZE_X as usize],
          }
      }

      pub fn from(mut binary_image_data: Vec<u8>) -> Result<Box<Self>, Box<Self>>  {
          let mut tmp: Box<std::mem::MaybeUninit<Self>> = Box::new_uninit();
          let tmp_ptr: *mut Self = tmp.as_mut_ptr();
          let mut success: bool = false;

          unsafe {
              success = decode_image_data(binary_image_data.as_mut_ptr(), binary_image_data.len().try_into().unwrap(), tmp_ptr);
          }

          if success {
              unsafe {
                  return Ok(tmp.assume_init());
              }
          } else {
              println!("Decode failed, returning 0");
              let size_in_bytes = std::mem::size_of::<Self>();
              unsafe {
                  std::ptr::write_bytes(tmp_ptr as *mut u8, 0, size_in_bytes);
                  Err(tmp.assume_init())
              }
          }
      }
  }

  impl Default for arg_input {
      fn default() -> Self {
          arg_input::new()
      }
  }
#+end_src

**** Implement methods for output class

***** List of output labels
#+begin_src rust :mkdirp yes :tangle ./src/model.rs
  const CLASS_LABELS: [&str; SIZE_O as usize] = ["empty", "occupied", "other"];
#+end_src

***** Actual functions
#+begin_src rust :mkdirp yes :tangle ./src/model.rs
  impl arg_output {

      pub fn new() -> Self {
          arg_output {
              val: [0 as outtype; SIZE_O as usize],
          }
      }

      pub fn from<T: std::ops::Index<usize, Output = outtype>>(input: T) -> Self {
          let mut ret = arg_output::new();
          for i in 0..SIZE_O {
              ret.val[i as usize] = input[i as usize];
          }
          ret
      }

  }

  impl Default for arg_output {
      fn default() -> Self {
          arg_output::new()
      }
  }
#+end_src

**** Main inference function
#+begin_src rust :mkdirp yes :tangle ./src/model.rs
  fn run_inference(input: &mut Vec<arg_input>) -> Vec<arg_output> {

      let mut output = Vec::<arg_output>::with_capacity(input.len());

      unsafe {
          mylibtorchinfer(input.as_mut_ptr(), input.len() as u32, output.as_mut_ptr());
          output.set_len(input.len())
      }

      return output
  }
#+end_src

**** Reply class

***** Actual structure
#+begin_src rust :mkdirp yes :tangle ./src/model.rs
  #[derive(serde::Serialize)]
  pub struct prediction_probabilities_reply {
      val: [String; SIZE_O as usize],
      mj: String,
  }
#+end_src

***** Methods
#+begin_src rust :mkdirp yes :tangle ./src/model.rs
  impl prediction_probabilities_reply {
      pub fn new() -> Self {
          prediction_probabilities_reply {
              val: std::array::from_fn(|_| String::new()),
              mj: String::new(),
          }
      }

      pub fn from(input: arg_output) -> prediction_probabilities_reply {
          let mut max_index: usize = 0;
          let mut ret = prediction_probabilities_reply::new();

          ret.val[0] = input.val[0].to_string();
          for i in 1..SIZE_O {
              ret.val[i as usize] = input.val[i as usize].to_string();
              if input.val[i as usize] > input.val[max_index] {
                  max_index = i as usize;
              }
          }

          ret.mj = CLASS_LABELS[max_index].to_string();

          return ret;
      }
  }
#+end_src

**** Struct to send the inference request to the inferring thread
#+begin_src rust :mkdirp yes :tangle ./src/model.rs
  pub struct InferRequest {
      img: Box<arg_input>,
      resp_tx: tokio::sync::oneshot::Sender<Result<arg_output, String>>,
  }
#+end_src

**** Model server

***** Actual struct
#+begin_src rust :mkdirp yes :tangle ./src/model.rs
  pub struct model_server {
      rx: tokio::sync::mpsc::Receiver<InferRequest>,
  }
#+end_src

***** Important configs
#+begin_src rust :mkdirp yes :tangle ./src/model.rs
  const MAX_BATCH: usize = 16 as usize;
  const BATCH_TIMEOUT: std::time::Duration = std::time::Duration::from_millis(200);
#+end_src

***** Methods
#+begin_src rust :mkdirp yes :tangle ./src/model.rs
  impl model_server {
      pub async fn infer_loop(&mut self) {
          let mut images = Vec::with_capacity(MAX_BATCH);

          while let Some(first) = self.rx.recv().await {

              let mut reply_channel = Vec::with_capacity(MAX_BATCH);
              reply_channel.push(first.resp_tx);

              images.push(*(first.img));

              let start = tokio::time::Instant::now();
              while images.len() < MAX_BATCH && start.elapsed() < BATCH_TIMEOUT {
                  match self.rx.try_recv() {
                      Ok(req) => {
                          images.push(*(req.img));
                          reply_channel.push(req.resp_tx);
                      },
                      Err(_) => break,
                  }
              }

              let outputs = run_inference(&mut images) ;
              images.clear();

              for (out, req) in outputs.into_iter().zip(reply_channel.into_iter()) {
                  let _ = req.send(Ok(out));
              }
          }
      }
  }
#+end_src

**** The model client

***** The Struct
#+begin_src rust :mkdirp yes :tangle ./src/model.rs
  pub struct model_client {
      tx: tokio::sync::mpsc::Sender<InferRequest>,
      preprocess: crate::mylib::image_processor,
  }
#+end_src

***** Implement the methods
#+begin_src rust :mkdirp yes :tangle ./src/model.rs
  impl model_client {
      pub async fn do_infer(
          &self,
          mut binary_image_data: Vec<u8>
      ) -> Result<arg_output, String> {

          let (resp_tx, resp_rx) = tokio::sync::oneshot::channel();

          let img = match arg_input::from(binary_image_data) {
              Ok(O) => {O} ,
              Err(E) => {println!("Failed to decode image, using blind 0s");E}
          } ;

          match self.tx.send(InferRequest{img: img, resp_tx: resp_tx}).await {
              Ok(_) => match resp_rx.await {
                  Ok(Ok(pred)) => {
                      return Ok(pred);
                  }
                  Ok(Err(e)) => {
                      return Err(e);
                  }
                  Err(e) => {
                      return Err("Recv Error".to_string());
                  }
              },
              Err(e) => {
                  return Err("Send error".to_string());
              }
          }
      }

      pub async fn do_infer_data(&self, data: Vec<u8>) -> Result<arg_output, String> {
          return self.do_infer(data).await;
      }
  }
#+end_src

**** Convenient method to get both the server and client
#+begin_src rust :mkdirp yes :tangle ./src/model.rs
  pub fn get_inference_tuple() -> (model_server, model_client) {
      let (tx, rx) = tokio::sync::mpsc::channel::<InferRequest>(512);
      let ret_server = model_server {
          rx: rx,
      };
      let ret_client = model_client {
          tx: tx,
          preprocess: crate::mylib::image_processor::new(IMAGE_RESOLUTION),
      };
      return (ret_server, ret_client);
  }
#+end_src

*** Main code

**** Change default allocator

***** mimalloc
#+begin_src rust :mkdirp yes :tangle ./src/main.rs
  use mimalloc::MiMalloc;

  #[global_allocator]
  static GLOBAL: MiMalloc = MiMalloc;
#+end_src

***** COMMENT Jemalloc
#+begin_src rust :mkdirp yes :tangle ./src/main.rs
  #[cfg(not(target_env = "msvc"))]
  use tikv_jemallocator::Jemalloc;

  #[cfg(not(target_env = "msvc"))]
  #[global_allocator]
  static GLOBAL: Jemalloc = Jemalloc;
#+end_src

***** COMMENT rpmalloc
#+begin_src rust :mkdirp yes :tangle ./src/main.rs
  use rpmalloc::RpMalloc;

  #[global_allocator]
  static ALLOC: RpMalloc = RpMalloc;
#+end_src

**** use lines
#+begin_src rust :mkdirp yes :tangle ./src/main.rs
  mod model;
  mod mylib;
  use futures_util::TryStreamExt;
#+end_src

**** Main functions

***** Actix
#+begin_src rust :mkdirp yes :tangle ./src/main.rs
  async fn infer_handler(
      mut payload: actix_multipart::Multipart,
      infer_slave: actix_web::web::Data<std::sync::Arc<model::model_client>>,
  ) -> Result<actix_web::HttpResponse, actix_web::Error> {

      let mut data = Vec::new();

      while let Some(mut field) = payload.try_next().await? {
          while let Some(chunk) = field.try_next().await? {
              data.extend_from_slice(&chunk);
          }
      }

      if data.is_empty() {
          return Ok(actix_web::HttpResponse::BadRequest().body("No image data"));
      }

      match infer_slave.do_infer_data(data).await {
          Ok(pred) => {
              return Ok(actix_web::HttpResponse::Ok().json(model::prediction_probabilities_reply::from(pred)));
          },
          Err(e) => {
              return Ok(actix_web::HttpResponse::InternalServerError().body(e));
          },
      }
  }
#+end_src

***** Tonic
#+begin_src rust :mkdirp yes :tangle ./src/main.rs
  pub mod infer {
      tonic::include_proto!("infer");
  }

  pub struct MyInferer {
      slave_client: std::sync::Arc<model::model_client>
  }

  #[tonic::async_trait]
  impl infer::infer_server::Infer for MyInferer {

      async fn do_infer(&self, request: tonic::Request<infer::Image>) -> Result<tonic::Response<infer::Prediction>, tonic::Status> {
          let image_data = request.into_inner().image_data;
          match self.slave_client.do_infer_data(image_data).await {
              Ok(pred) => {
                  let reply = infer::Prediction {
                      ps1: pred.val[0],
                      ps2: pred.val[1],
                      ps3: pred.val[2],
                  };
                  return Ok(tonic::Response::new(reply));
              },
              Err(e) => {
                  Err(tonic::Status::internal(e))
              },
          }
      }
  }
#+end_src

**** Actual functions to start actix and tonic

***** Actix
#+begin_src rust :mkdirp yes :tangle ./src/main.rs
  async fn main_actix(slave_client_1: std::sync::Arc<crate::model::model_client>) -> () {
      let port: u16 = 8000;
      match actix_web::HttpServer::new(
          move || {
            actix_web::App::new()
                .app_data(actix_web::web::Data::new(std::sync::Arc::clone(&slave_client_1)))
                .route("/infer", actix_web::web::post().to(infer_handler))
          }
      ).bind(("0.0.0.0", port)) {
          Ok(ret) => {
              println!("Actix binding to port {}", port);
              ret.run().await;
          }
          Err(e) => {
              eprintln!("Failed to bind to port");
          }
      }
  }
#+end_src

***** Tonic
#+begin_src rust :mkdirp yes :tangle ./src/main.rs
  async fn main_tonic(slave_client_2: std::sync::Arc<crate::model::model_client>) {
      let ip_v4 = std::net::IpAddr::V4(std::net::Ipv4Addr::new(0, 0, 0, 0));
      let port: u16 = 8001;
      let addr = std::net::SocketAddr::new(ip_v4, port);
      let inferer_service = MyInferer{slave_client: slave_client_2};
      println!("Starting tonic grpc server at port {}",port);
      tonic::transport::Server::builder().add_service(infer::infer_server::InferServer::new(inferer_service)).serve(addr).await;
  }
#+end_src

**** Simpler main
#+begin_src rust :mkdirp yes :tangle ./src/main.rs
  fn main() {
      let (mut slave_server, slave_client) = crate::model::get_inference_tuple();

      let rt = tokio::runtime::Builder::new_multi_thread()
          .thread_stack_size(1 << 24) 
          .enable_all()
          .build()
          .unwrap();

      rt.block_on(async {
          let slave_client_1 = std::sync::Arc::new(slave_client);
          let slave_client_2 = std::sync::Arc::clone(&slave_client_1);
          let future_infer = slave_server.infer_loop();
          let future_actix = main_actix(slave_client_1);
          let future_tonic = main_tonic(slave_client_2);
          tokio::join!(future_infer, future_actix, future_tonic);
      });
  }
#+end_src

* Script to run the rust code

** Add the file to git
#+begin_src sh :shebang #!/bin/sh :results output :mkdirp yes :tangle ./.git.sh
  GITADD 'run.sh'
  GITADD 'infer.sh'
#+end_src

** Run server
#+begin_src sh :shebang #!/bin/sh :results output :mkdirp yes :tangle ./run.sh
  cd "$(dirname -- "${0}")"
  export ROCR_VISIBLE_DEVICES=0
  './source/compile.sh'
  exec infer-server
#+end_src

** Run Client
#+begin_src sh :shebang #!/bin/sh :results output :mkdirp yes :tangle ./infer.sh
  cd '/data/source'
  infer-client
#+end_src

* Script to compile everything

** Add the file to git
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./.git.sh
  GITADD 'compile.sh'
#+end_src

** Actual script

*** Header
#+begin_src sh :shebang #!/bin/sh :results output :mkdirp yes :tangle ./compile.sh
  cd "$(dirname -- "${0}")"
  SRC="$(realpath .)"
  BLD="${SRC}/../build"
  mkdir -pv -- "${BLD}"
  BLD="$('realpath' "${BLD}")"
#+end_src

*** COMMENT Build c++
#+begin_src sh :shebang #!/bin/sh :results output :mkdirp yes :tangle ./compile.sh
  cd "${BLD}"
  cmake "${SRC}"
  make -j4
  make install
#+end_src

*** COMMENT Build rust
#+begin_src sh :shebang #!/bin/sh :results output :mkdirp yes :tangle ./compile.sh
  cd "${SRC}"
  export RUSTFLAGS="-C target-cpu=native"
  export CARGO_TARGET_DIR="${BLD}/cargo"
  mkdir -pv -- "${CARGO_TARGET_DIR}"
  bindgen './src/export.hpp' > './src/export.rs'
  cargo build --bin infer-server --release
  cargo build --bin infer-client --release
  cp -vf -- "${CARGO_TARGET_DIR}/release/infer-server" "${BLD}/"
  mkdir -pv -- '/usr/bin/'
  install --compare  "${CARGO_TARGET_DIR}/release/infer-server" '/usr/bin/infer-server'
  install --compare  "${CARGO_TARGET_DIR}/release/infer-client" '/usr/bin/infer-client'
#+end_src

*** Build pytorch model
#+begin_src sh :shebang #!/bin/sh :results output :mkdirp yes :tangle ./compile.sh
  cd "${SRC}"
  H="$(cat ./main.py | sha512sum | cut -d ' ' -f1)"
  mkdir -pv -- "${HOME}/.cache/${H}"
  test -e "${HOME}/.cache/${H}/model.pt2" || './compile_2_trt.py' './model.pt2' "${HOME}/.cache/${H}/model.pt2"
  # test -e "${HOME}/.cache/${H}/model.pt2" || ./compile.py './model.ckpt' "${HOME}/.cache/${H}/model.pt2"
  ln -vfs -- "${HOME}/.cache/${H}/model.pt2" '/model.pt2'
#+end_src

*** Exit the script
#+begin_src sh :shebang #!/bin/sh :results output :mkdirp yes :tangle ./compile.sh
  exit '0'
#+end_src

* COMMENT Work space

** Just add
#+begin_src emacs-lisp :results silent
  (save-buffer)
  (org-babel-tangle)
  (async-shell-command "
      './.git.sh'
  " "log" "err")
#+end_src

** add commit and push
#+begin_src emacs-lisp :results silent
  (save-buffer)
  (org-babel-tangle)
  (async-shell-command "
      './.git.sh'
      git commit -m 'more trt debugging'
      $HOME/SSH/KEYS/PERSONAL_LAPTOP_PERSONAL_GITHUB/setup.sh
      git push
  " "log" "err")
#+end_src
